{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01d42dee",
   "metadata": {},
   "source": [
    "## Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b140c8",
   "metadata": {},
   "source": [
    "Reinforcement learning considers the situation where a series of decisions have to be made in a\n",
    "stochastically changing environment. At the time of each decision, there are a number of ***states***\n",
    "and a number of possible ***actions***.  \n",
    "\n",
    "The decision maker takes an action, $A_0$, at time zero when the state $S_0$ is known. This results in a reward, $R_1$, at time $t=1$ and a new state, $S_1$, is then encountered. The decision maker then takes another action, $A_1$ which\n",
    "results in a reward, $R_2$ at time $t=2$ and a new state, $S_2$; and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b78ebe1",
   "metadata": {},
   "source": [
    "The aim of reinforcement learning is to maximize expected future rewards. Specifically, it attempts\n",
    "to maximize the expected value of $G_t$ where\n",
    "\n",
    "\\begin{equation}\\label{eqn_1}\n",
    "G_t =R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} +\\dots+\\gamma^{T-1}R_T\n",
    "\\end{equation}\n",
    "\n",
    "$T$ is a horizon date and  $\\gamma \\in (0, 1]$ is a discount factor. To ensure that equation \\eqref{eqn_1} reflects the time value of money, we define $R_t$ as the the cash flow received at time $t$ multiplied by $\\gamma$ (i.e., discounted\n",
    "by one period). \n",
    "\n",
    "In order to maximize $G_t$ in equation \\eqref{eqn_1}, the decision maker needs a set of rules for what action\n",
    "to take in any given state. This set of rules is represented by a policy function $\\pi : \\cal{S} \\rightarrow \\cal{A}$, where $\\cal{S}$ and $\\cal{A}$ are the sets of all possible ***states*** and ***actions***, respectively. \n",
    "\n",
    "If the decision maker uses policy $\\pi$ and is in state $S_t$ at time $t$, then the action taken is $A_t =\\pi(S_t)$. The policy is updated as the reinforcement learning algorithm progresses. As we explain later, learning an optimal policy\n",
    "involves both ***exploration*** and ***exploitation***.\n",
    "\n",
    "For example, at a particular stage in the execution of the reinforcement learning algorithm, the policy might involve, for all states and all times, a 90% chance of taking the best action identified so far (***exploitation***) and a 10% chance of randomly selecting a different action (***exploration***)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687e87b4",
   "metadata": {},
   "source": [
    "For a specific policy $\\pi$, we define the value of taking action $A_t$ in a state $S_t$ as the expected total\n",
    "reward (discounted) starting from state $S_t$ taking action $A_t$ and taking the actions given by the\n",
    "policy in the future states that are encountered. \n",
    "\n",
    "The value of each state-action pair is represented by a function $Q: \\cal{S} \\times \\cal{A}\\rightarrow \\mathbb{R}$, referred to as the action-value function or ***Q-function***:\n",
    "\n",
    "\\begin{equation}\n",
    "Q(S_t;A_t)= \\mathbb{E}(G_t \\vert S_t;A_t)\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560984ae",
   "metadata": {},
   "source": [
    "The two core stages of the reinforcement learning process are: \n",
    "\n",
    "- the ***estimation of the Q-function*** and \n",
    "- the ***policy update***."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b281cc7",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sum\\limits_a \\, \\pi(a \\vert s) \\sum\\limits_{s^\\prime}\\sum\\limits_r \\, p(s^\\prime, r \\vert s, a) =1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324eee7f",
   "metadata": {},
   "source": [
    "### Temporal Difference Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b264c2b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d44c0d0e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2176b8aa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aaef4d2b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95b9d1b9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7884535e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14bf6f68",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "904e9fe9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "baeb1a4f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fea94172",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a617f89",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e963b49c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
