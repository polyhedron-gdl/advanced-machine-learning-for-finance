{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3134ad44",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/polyhedron-gdl/advanced-machine-learning-for-finance/blob/main/2022/1-notebooks/chapter-4-2.ipynb\">\n",
    "        <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7642c3e7",
   "metadata": {},
   "source": [
    "# Text Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ecc891e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import numpy  as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70791adb",
   "metadata": {},
   "source": [
    "## What are Word Vectorization and Why It is so Important\n",
    "\n",
    "Word vectorization generically refers to techniques used to convert text into numbers. There may be different numerical representations of the same text. \n",
    "\n",
    "Many Machine Learning algorithms and almost all Deep Learning Architectures are incapable of processing *strings* or *plain text* in their raw form. They require numbers as inputs to perform any sort of job, be it classification, regression, etc. in broad terms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39baeec2",
   "metadata": {},
   "source": [
    "## Bag-of-Words (BOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c1af1f",
   "metadata": {},
   "source": [
    "As we have already said, in order to perform machine learning on text, we need to transform our documents into vector representations such that we can apply numeric machine learning. This process is called feature extraction or more simply, vectorization.\n",
    "\n",
    "We will explore several choices, each of which extends or modifies the base bag-of-words model to describe semantic space. We will look at four types of vector encoding - frequency, one-hot, TF-IDF."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2623ce0d",
   "metadata": {},
   "source": [
    "### Sample corpus of text documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4c9e83",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img align=\"left\" width=\"100\" height=\"100\" src=\"./pic/text_analytics_with_python.jpg\"/> </td>\n",
    "<td> The following examples are taken from \"Text Analytics with Python\" by Dipanjan Sarkar (Apress, 2019).     </td>\n",
    "</tr></table>\n",
    "\n",
    "[Here](https://github.com/Apress/text-analytics-w-python-2e) you can find the original notebook used in the aforementioned textbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69d9b5d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The sky is blue and beautiful.</td>\n",
       "      <td>weather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Love this blue and beautiful sky!</td>\n",
       "      <td>weather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The quick brown fox jumps over the lazy dog.</td>\n",
       "      <td>animals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A king's breakfast has sausages, ham, bacon, e...</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I love green eggs, ham, sausages and bacon!</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The brown fox is quick and the blue dog is lazy!</td>\n",
       "      <td>animals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The sky is very blue and the sky is very beaut...</td>\n",
       "      <td>weather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The dog is lazy but the brown fox is quick!</td>\n",
       "      <td>animals</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Document Category\n",
       "0                     The sky is blue and beautiful.  weather\n",
       "1                  Love this blue and beautiful sky!  weather\n",
       "2       The quick brown fox jumps over the lazy dog.  animals\n",
       "3  A king's breakfast has sausages, ham, bacon, e...     food\n",
       "4        I love green eggs, ham, sausages and bacon!     food\n",
       "5   The brown fox is quick and the blue dog is lazy!  animals\n",
       "6  The sky is very blue and the sky is very beaut...  weather\n",
       "7        The dog is lazy but the brown fox is quick!  animals"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = ['The sky is blue and beautiful.',\n",
    "          'Love this blue and beautiful sky!',\n",
    "          'The quick brown fox jumps over the lazy dog.',\n",
    "          \"A king's breakfast has sausages, ham, bacon, eggs, toast and beans\",\n",
    "          'I love green eggs, ham, sausages and bacon!',\n",
    "          'The brown fox is quick and the blue dog is lazy!',\n",
    "          'The sky is very blue and the sky is very beautiful today',\n",
    "          'The dog is lazy but the brown fox is quick!'    \n",
    "]\n",
    "labels = ['weather', 'weather', 'animals', 'food', 'food', 'animals', 'weather', 'animals']\n",
    "\n",
    "corpus = np.array(corpus)\n",
    "corpus_df = pd.DataFrame({'Document': corpus, \n",
    "                          'Category': labels})\n",
    "corpus_df = corpus_df[['Document', 'Category']]\n",
    "corpus_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca7f982",
   "metadata": {},
   "source": [
    "### Simple text pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd63a0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "wpt = nltk.WordPunctTokenizer()\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def normalize_document(doc):\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    # tokenize document\n",
    "    tokens = wpt.tokenize(doc)\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "\n",
    "normalize_corpus = np.vectorize(normalize_document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0614c70",
   "metadata": {},
   "source": [
    "**Regular Expression Flags**\n",
    "\n",
    "**re.I - \n",
    "re.IGNORECASE**\n",
    "\n",
    "Perform case-insensitive matching; expressions like [A-Z] will also match lowercase letters. Full Unicode matching (such as Ü matching ü) also works unless the re.ASCII flag is used to disable non-ASCII matches. The current locale does not change the effect of this flag unless the re.LOCALE flag is also used. Corresponds to the inline flag (?i).\n",
    "\n",
    "**re.A - \n",
    "re.ASCII**\n",
    "\n",
    "Make \\\\w, \\\\W, \\\\b, \\\\B, \\\\d, \\\\D, \\\\s and \\\\S perform ASCII-only matching instead of full Unicode matching. This is only meaningful for Unicode patterns, and is ignored for byte patterns. Corresponds to the inline flag (?a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "963c9d14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['sky blue beautiful', 'love blue beautiful sky',\n",
       "       'quick brown fox jumps lazy dog',\n",
       "       'kings breakfast sausages ham bacon eggs toast beans',\n",
       "       'love green eggs ham sausages bacon',\n",
       "       'brown fox quick blue dog lazy', 'sky blue sky beautiful today',\n",
       "       'dog lazy brown fox quick'], dtype='<U51')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_corpus = normalize_corpus(corpus)\n",
    "norm_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890dd823",
   "metadata": {},
   "source": [
    "### Frequency Vectors\n",
    "\n",
    "The simplest vector encoding model is to simply fill in the vector with the frequency of each word as it appears in the document;\n",
    "In this encoding scheme each document is represented as the multiset of the tokens that compose it and the value for each word position in the vectr is its count;\n",
    "This representation can either be a straight count encoding or a normalized encoding where each word is weighted by the total number of words in the document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4092ad55",
   "metadata": {},
   "source": [
    "In **Scikit-Learn** The CountVectorizer transformer from the sklearn.feature_extraction model has its own internal tokenization and normalization methods. The fit method of the vectorizer expects an iterable or list of strings or file objects, and creates a dictionary of the vocabulary on the corpus. When transform is called, each individual document is transformed into a sparse array whose index tuple is the row (the document ID) and the token ID from the dictionary, and whose value is the count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29e38c50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<8x20 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 42 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# get bag of words features in sparse format\n",
    "cv = CountVectorizer(min_df=0., max_df=1.)\n",
    "cv_matrix = cv.fit_transform(norm_corpus)\n",
    "cv_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "baccd31d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_matrix = cv_matrix.toarray()\n",
    "cv_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a70d2ee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bacon</th>\n",
       "      <th>beans</th>\n",
       "      <th>beautiful</th>\n",
       "      <th>blue</th>\n",
       "      <th>breakfast</th>\n",
       "      <th>brown</th>\n",
       "      <th>dog</th>\n",
       "      <th>eggs</th>\n",
       "      <th>fox</th>\n",
       "      <th>green</th>\n",
       "      <th>ham</th>\n",
       "      <th>jumps</th>\n",
       "      <th>kings</th>\n",
       "      <th>lazy</th>\n",
       "      <th>love</th>\n",
       "      <th>quick</th>\n",
       "      <th>sausages</th>\n",
       "      <th>sky</th>\n",
       "      <th>toast</th>\n",
       "      <th>today</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bacon  beans  beautiful  blue  breakfast  brown  dog  eggs  fox  green  \\\n",
       "0      0      0          1     1          0      0    0     0    0      0   \n",
       "1      0      0          1     1          0      0    0     0    0      0   \n",
       "2      0      0          0     0          0      1    1     0    1      0   \n",
       "3      1      1          0     0          1      0    0     1    0      0   \n",
       "4      1      0          0     0          0      0    0     1    0      1   \n",
       "5      0      0          0     1          0      1    1     0    1      0   \n",
       "6      0      0          1     1          0      0    0     0    0      0   \n",
       "7      0      0          0     0          0      1    1     0    1      0   \n",
       "\n",
       "   ham  jumps  kings  lazy  love  quick  sausages  sky  toast  today  \n",
       "0    0      0      0     0     0      0         0    1      0      0  \n",
       "1    0      0      0     0     1      0         0    1      0      0  \n",
       "2    0      1      0     1     0      1         0    0      0      0  \n",
       "3    1      0      1     0     0      0         1    0      1      0  \n",
       "4    1      0      0     0     1      0         1    0      0      0  \n",
       "5    0      0      0     1     0      1         0    0      0      0  \n",
       "6    0      0      0     0     0      0         0    2      0      1  \n",
       "7    0      0      0     1     0      1         0    0      0      0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all unique words in the corpus\n",
    "vocab = cv.get_feature_names_out()\n",
    "# show document feature vectors\n",
    "pd.DataFrame(cv_matrix, columns=vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad73797",
   "metadata": {},
   "source": [
    "\n",
    "### One-Hot Encoding\n",
    "\n",
    "Because they disregard grammar and the relative position of words in documents, frequency-based encoding methods suffer from the long tail, or Zipfian distribution, that characterizes natural language. As a result, tokens that occur very frequently are orders of magnitude more “significant” than other, less frequent ones. This can have a significant impact on some models (e.g., generalized linear models) that expect normally distributed features. \n",
    "\n",
    "A solution to this problem is **one-hot encoding**, a boolean vector encoding method that marks a particular vector index with a value of true (1) if the token exists in the document and false (0) if it does not. In other words, each element of a one-hot encoded vector reflects either the presence or absence of the token in the described text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "384d7da5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<8x20 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 42 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get bag of words features in sparse format\n",
    "cv = CountVectorizer(min_df=0., max_df=1., binary=True)\n",
    "cv_matrix = cv.fit_transform(norm_corpus)\n",
    "cv_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b97e877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_matrix = cv_matrix.toarray()\n",
    "cv_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808b67cd",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**TF - Term Frequency**: Just counting the number of words in each document has 1 issue: it will give more weightage to longer documents than shorter documents. To avoid this, we can use frequency i.e. #count(word) / #Total words, in each document.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d3cb15",
   "metadata": {},
   "source": [
    "### Term Frequency-Inverse Document Frequency\n",
    "\n",
    "The bag-of-words representations that we have explored so far only describe a document in a standalone fashion, not taking into account the context of the corpus. \n",
    "A better approach would be to consider the relative frequency or rareness of tokens in the document against their frequency in other documents. The central insight is that meaning is most likely encoded in the more rare terms from a document.\n",
    "\n",
    "TF-IDF, term frequency-inverse document frequency, encoding normalizes the frequency of tokens in a document with respect to the rest of the corpus. \n",
    "This encoding approach accentuates terms that are very relevant to a specific instance, as shown in Figure, where the token studio has a higher relevance to this document since it only appears there.\n",
    "\n",
    "The inverse document frequency is a measure of how much information the word provides, that is, whether the term is common or rare across all documents. \n",
    "It is the logarithmically scaled inverse fraction of the documents that contain the word, obtained by dividing the total number of documents by the number of documents containing the term, and then taking the logarithm of that quotient: \n",
    "\n",
    "\\begin{equation}idf(t,D) = \\log \\frac{N}{\\vert \\{  d \\in D: t \\in d\\}\\vert} \\end{equation}  \n",
    "\n",
    "where the numerator ($N$) is the total number of documents in the corpus and the denominator is the number of documents where the term $t$ appears.\n",
    "\n",
    "Then tf-idf is calculated as: \n",
    "\n",
    "\\begin{equation} tfidf(t,d,D) = tf(t,d) \\cdot idf(t,D)\\end{equation}\n",
    "\n",
    "A high weight in tf-idf is reached by a high term frequency (in the given document) and a low document frequency of the term in the whole collection of documents; the weights hence tend to filter out common terms.  Since the ratio inside the idf's log function is always greater than or equal to 1, the value of idf (and tf-idf) is greater than or equal to 0. As a term appears in more documents, the ratio inside the logarithm approaches 1, bringing the idf and tf-idf closer to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1a3b0b",
   "metadata": {},
   "source": [
    "![image.png](./pic/tf-idf-0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344bcc72",
   "metadata": {},
   "source": [
    "A simple example might serve to explain the structure of the TDM more clearly. Assume we have a simple corpus consisting of two documents, Doc1 and Doc2, with the following content:\n",
    "\n",
    "Doc1 = \"I like databases\"\n",
    "Doc2 = \"I dislike databases\",\n",
    "\n",
    "then the document-term matrix would be:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f668d879",
   "metadata": {},
   "source": [
    "![image.png](./pic/tf-idf-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccfa1ff",
   "metadata": {},
   "source": [
    "Clearly there is nothing special about rows and columns – we could just as easily transpose them. If we did so, we’d get a term document matrix (TDM) in which the terms are rows and documents columns. One can work with either a DTM or TDM. Using the raw count of a term in a document, i.e. the number of times that term t occurs in document d, is the simplest choice to measure the term frequency $tf(t,d)$. If we denote the raw count by $f_{t,d}$, then the simplest $tf$ scheme is $tf(t,d) = f_{t,d}$. Other possibilities include\n",
    "\n",
    "- Boolean \"frequencies\": $tf(t,d) = 1$ if $t$ occurs in $d$ and $0$ otherwise;\n",
    "- Term frequency adjusted for document length : $f_{t,d} \\big/ \\text{(number of words in d)}$;\n",
    "- Logarithmically scaled frequency: $tf(t,d) = \\log (1 + f_{t,d})$;\n",
    "- Augmented frequency, to prevent a bias towards longer documents, e.g. raw frequency divided by the raw frequency of the most occurring term in the document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1cfa05",
   "metadata": {},
   "source": [
    "![image.png](./pic/3_1_text_vectorization_pic_6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ef75c7",
   "metadata": {},
   "source": [
    "Scikit-Learn provides a transformer called the TfidfVectorizer in the module called **feature_extraction.text** for vectorizing documents with TF–IDF scores. Under the hood, the TfidfVectorizer uses the CountVectorizer estimator we used to produce the bag-of-words encoding to count occurrences of tokens, followed by a TfidfTransformer, which normalizes these occurrence counts by the inverse document frequency. \n",
    "\n",
    "The input for a TfidfVectorizer is expected to be a sequence of filenames, file-like objects, or strings that contain a collection of raw documents, similar to that of the CountVectorizer. As a result, a default tokenization and preprocessing method is applied unless other functions are specified. The vectorizer returns a sparse matrix representation in the form of ((doc, term), tfidf) where each key is a document and term pair and the value is the TF–IDF score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61a5b843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bacon</th>\n",
       "      <th>beans</th>\n",
       "      <th>beautiful</th>\n",
       "      <th>blue</th>\n",
       "      <th>breakfast</th>\n",
       "      <th>brown</th>\n",
       "      <th>dog</th>\n",
       "      <th>eggs</th>\n",
       "      <th>fox</th>\n",
       "      <th>green</th>\n",
       "      <th>ham</th>\n",
       "      <th>jumps</th>\n",
       "      <th>kings</th>\n",
       "      <th>lazy</th>\n",
       "      <th>love</th>\n",
       "      <th>quick</th>\n",
       "      <th>sausages</th>\n",
       "      <th>sky</th>\n",
       "      <th>toast</th>\n",
       "      <th>today</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.32</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bacon  beans  beautiful  blue  breakfast  brown   dog  eggs   fox  green  \\\n",
       "0   0.00   0.00       0.60  0.53       0.00   0.00  0.00  0.00  0.00   0.00   \n",
       "1   0.00   0.00       0.49  0.43       0.00   0.00  0.00  0.00  0.00   0.00   \n",
       "2   0.00   0.00       0.00  0.00       0.00   0.38  0.38  0.00  0.38   0.00   \n",
       "3   0.32   0.38       0.00  0.00       0.38   0.00  0.00  0.32  0.00   0.00   \n",
       "4   0.39   0.00       0.00  0.00       0.00   0.00  0.00  0.39  0.00   0.47   \n",
       "5   0.00   0.00       0.00  0.37       0.00   0.42  0.42  0.00  0.42   0.00   \n",
       "6   0.00   0.00       0.36  0.32       0.00   0.00  0.00  0.00  0.00   0.00   \n",
       "7   0.00   0.00       0.00  0.00       0.00   0.45  0.45  0.00  0.45   0.00   \n",
       "\n",
       "    ham  jumps  kings  lazy  love  quick  sausages   sky  toast  today  \n",
       "0  0.00   0.00   0.00  0.00  0.00   0.00      0.00  0.60   0.00    0.0  \n",
       "1  0.00   0.00   0.00  0.00  0.57   0.00      0.00  0.49   0.00    0.0  \n",
       "2  0.00   0.53   0.00  0.38  0.00   0.38      0.00  0.00   0.00    0.0  \n",
       "3  0.32   0.00   0.38  0.00  0.00   0.00      0.32  0.00   0.38    0.0  \n",
       "4  0.39   0.00   0.00  0.00  0.39   0.00      0.39  0.00   0.00    0.0  \n",
       "5  0.00   0.00   0.00  0.42  0.00   0.42      0.00  0.00   0.00    0.0  \n",
       "6  0.00   0.00   0.00  0.00  0.00   0.00      0.00  0.72   0.00    0.5  \n",
       "7  0.00   0.00   0.00  0.45  0.00   0.45      0.00  0.00   0.00    0.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tv = TfidfVectorizer(min_df=0., max_df=1., norm='l2',\n",
    "                     use_idf=True, smooth_idf=True)\n",
    "tv_matrix = tv.fit_transform(norm_corpus)\n",
    "tv_matrix = tv_matrix.toarray()\n",
    "\n",
    "vocab = tv.get_feature_names_out()\n",
    "pd.DataFrame(np.round(tv_matrix, 2), columns=vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad14054a",
   "metadata": {},
   "source": [
    "### Extracting Features for New Documents\n",
    "\n",
    "Suppose you built a machine learning model to classify and categorize news articles and it is currently in production. How you can generate features for completely new documents so that you can feed it into the machine learning models for prediction? The Scikit-Learn API provides the `transform` function for the vectorizers we discussed previously and we can leverage it to ge features for a completely new document that was not present in our corpus. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73276e97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bacon</th>\n",
       "      <th>beans</th>\n",
       "      <th>beautiful</th>\n",
       "      <th>blue</th>\n",
       "      <th>breakfast</th>\n",
       "      <th>brown</th>\n",
       "      <th>dog</th>\n",
       "      <th>eggs</th>\n",
       "      <th>fox</th>\n",
       "      <th>green</th>\n",
       "      <th>ham</th>\n",
       "      <th>jumps</th>\n",
       "      <th>kings</th>\n",
       "      <th>lazy</th>\n",
       "      <th>love</th>\n",
       "      <th>quick</th>\n",
       "      <th>sausages</th>\n",
       "      <th>sky</th>\n",
       "      <th>toast</th>\n",
       "      <th>today</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bacon  beans  beautiful  blue  breakfast  brown  dog  eggs  fox  green  \\\n",
       "0    0.0    0.0        0.0   0.0        0.0    0.0  0.0   0.0  0.0   0.63   \n",
       "\n",
       "   ham  jumps  kings  lazy  love  quick  sausages   sky  toast  today  \n",
       "0  0.0    0.0    0.0   0.0   0.0    0.0       0.0  0.46    0.0   0.63  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_doc = 'the sky is green today'\n",
    "\n",
    "pd.DataFrame(np.round(tv.transform([new_doc]).toarray(), 2), \n",
    "             columns=tv.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58868fea",
   "metadata": {},
   "source": [
    "## Document Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de4e2a5",
   "metadata": {},
   "source": [
    "When you have vectorized your text, we can try to define a distance metric such that documents that are closer together in feature space are more similar. There are a number of different measures that can be used to determine document similarity. Fundamentally, each relies on our ability to imagine documents as points in space, where the relative closeness of any two documents is a measure of their similarity.\n",
    "\n",
    "We can measure vector similarity with cosine distance, using the cosine of the angle between the two vectors to assess the degree to which they share the same orientation. In effect, the more parallel any two vectors are, the more similar the documents will be (regardless of their magnitude)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19fad09",
   "metadata": {},
   "source": [
    "<!--\n",
    "<div>\n",
    "<img src=\"./img/3_1_text_vectorization_pic_9.png\" width=\"500\"/>\n",
    "</div>\n",
    "-->\n",
    "![pic](./pic/3_1_text_vectorization_pic_9.png)\n",
    "\n",
    "*(image source: Bengfort B. et al. \"Text Analysis with Python\")*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0696c72",
   "metadata": {},
   "source": [
    "Mathematically, Cosine similarity metric measures the cosine of the angle between two n-dimensional vectors projected in a multi-dimensional space. The Cosine similarity of two documents will range from 0 to 1. If the Cosine similarity score is 1, it means two vectors have the same orientation. The value closer to 0 indicates that the two documents have less similarity.\n",
    "\n",
    "The mathematical equation of Cosine similarity between two non-zero vectors is: \n",
    "\n",
    "\\begin{equation} \\text{similarity} = \\cos(\\theta) = \\frac{\\mathbf{A} \\cdot \\mathbf{B}}{\\left\\Vert\\mathbf{A}\\right\\Vert \\,\\left\\Vert\\mathbf{B}\\right\\Vert} = \\frac{\\sum\\limits_{i=1}^n A_iB_i}{\\sqrt{\\sum\\limits_{i=1}^n A_i^2} \\sqrt{ \\sum\\limits_{i=1}^n B_i^2}} \\end{equation}\n",
    "\n",
    "Let’s see an example of how to calculate the cosine similarity between two text document. The common way to compute the Cosine similarity is to first we need to count the word occurrence in each document. To count the word occurrence in each document, we can use **CountVectorizer** or **TfidfVectorizer** functions that are provided by Scikit-Learn library.\n",
    "\n",
    "doc_1 = \"Data is the oil of the digital economy\" \n",
    "doc_2 = \"Data is a new oil\" \n",
    "\n",
    "and consider the following frequency matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45755e63",
   "metadata": {},
   "source": [
    "<!--\n",
    "<img src='./img/tf-idf-2.png'>\n",
    "-->\n",
    "![image.png](./pic/tf-idf-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b514f5",
   "metadata": {},
   "source": [
    "We can write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "982fc4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_1 = \"Data is the oil of the digital economy\" \n",
    "doc_2 = \"Data is a new oil\" \n",
    "data = [doc_1, doc_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "850c2508",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "vector_matrix = count_vectorizer.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248d0818",
   "metadata": {},
   "source": [
    "Here, is the unique tokens list found in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08d99a35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['data', 'digital', 'economy', 'is', 'new', 'of', 'oil', 'the'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = count_vectorizer.get_feature_names_out()\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3c0b206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 0, 1, 1, 2],\n",
       "       [1, 0, 0, 1, 1, 0, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c08bbe8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>digital</th>\n",
       "      <th>economy</th>\n",
       "      <th>is</th>\n",
       "      <th>new</th>\n",
       "      <th>of</th>\n",
       "      <th>oil</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>doc_1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       data  digital  economy  is  new  of  oil  the\n",
       "doc_1     1        1        1   1    0   1    1    2\n",
       "doc_2     1        0        0   1    1   0    1    0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_dataframe(matrix, tokens):\n",
    "\n",
    "    doc_names = [f'doc_{i+1}' for i, _ in enumerate(matrix)]\n",
    "    df = pd.DataFrame(data=matrix, index=doc_names, columns=tokens)\n",
    "    return(df)\n",
    "\n",
    "create_dataframe(vector_matrix.toarray(),tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1d78df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = vector_matrix.toarray()\n",
    "v1 = np.array(a[0])\n",
    "v2 = np.array(a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "651b00e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = np.linalg.norm(v1)\n",
    "n2 = np.linalg.norm(v2)\n",
    "prod = np.inner(v1,v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee7450eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4743416490252569\n"
     ]
    }
   ],
   "source": [
    "print(prod/(n1*n2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "edf97f16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_1</th>\n",
       "      <th>doc_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>doc_1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.474342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_2</th>\n",
       "      <td>0.474342</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          doc_1     doc_2\n",
       "doc_1  1.000000  0.474342\n",
       "doc_2  0.474342  1.000000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cosine_similarity_matrix = cosine_similarity(vector_matrix)\n",
    "create_dataframe(cosine_similarity_matrix,['doc_1','doc_2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bd9ca9",
   "metadata": {},
   "source": [
    "Let’s check the cosine similarity with `TfidfVectorizer`, and see how it change over `CountVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f05c0d41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>digital</th>\n",
       "      <th>economy</th>\n",
       "      <th>is</th>\n",
       "      <th>new</th>\n",
       "      <th>of</th>\n",
       "      <th>oil</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>doc_1</th>\n",
       "      <td>0.243777</td>\n",
       "      <td>0.34262</td>\n",
       "      <td>0.34262</td>\n",
       "      <td>0.243777</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.34262</td>\n",
       "      <td>0.243777</td>\n",
       "      <td>0.68524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_2</th>\n",
       "      <td>0.448321</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.448321</td>\n",
       "      <td>0.630099</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.448321</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           data  digital  economy        is       new       of       oil  \\\n",
       "doc_1  0.243777  0.34262  0.34262  0.243777  0.000000  0.34262  0.243777   \n",
       "doc_2  0.448321  0.00000  0.00000  0.448321  0.630099  0.00000  0.448321   \n",
       "\n",
       "           the  \n",
       "doc_1  0.68524  \n",
       "doc_2  0.00000  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "Tfidf_vect = TfidfVectorizer()\n",
    "vector_matrix = Tfidf_vect.fit_transform(data)\n",
    "\n",
    "tokens = Tfidf_vect.get_feature_names_out()\n",
    "create_dataframe(vector_matrix.toarray(),tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b92ce86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_1</th>\n",
       "      <th>doc_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>doc_1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.327871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_2</th>\n",
       "      <td>0.327871</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          doc_1     doc_2\n",
       "doc_1  1.000000  0.327871\n",
       "doc_2  0.327871  1.000000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity_matrix = cosine_similarity(vector_matrix)\n",
    "create_dataframe(cosine_similarity_matrix,['doc_1','doc_2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967774d5",
   "metadata": {},
   "source": [
    "Here, using `TfidfVectorizer` we get the cosine similarity between doc_1 and doc_2 is 0.32.  Where the `CountVectorizer` has returned the cosine similarity of doc_1 and doc_2 is 0.47. This is because `TfidfVectorizer` penalized the most frequent words in the document such as stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b859ced2",
   "metadata": {},
   "source": [
    "Now, the **distance** can be defined as \n",
    "\n",
    "\\begin{equation}\n",
    "d =1-\\mathrm{CosineSimilarity} \n",
    "\\end{equation}\n",
    "\n",
    "The intuition behind this is that if 2 vectors are perfectly the same then similarity is 1 (angle=0) and thus, distance is 0 (1-1=0).\n",
    "\n",
    "\n",
    "Let's apply the same analysis to our toy corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fcd759d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.820599</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.192353</td>\n",
       "      <td>0.817246</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.820599</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.225489</td>\n",
       "      <td>0.157845</td>\n",
       "      <td>0.670631</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.791821</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.850516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.506866</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.225489</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.506866</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.192353</td>\n",
       "      <td>0.157845</td>\n",
       "      <td>0.791821</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.115488</td>\n",
       "      <td>0.930989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.817246</td>\n",
       "      <td>0.670631</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.115488</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.850516</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.930989</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  1.000000  0.820599  0.000000  0.000000  0.000000  0.192353  0.817246   \n",
       "1  0.820599  1.000000  0.000000  0.000000  0.225489  0.157845  0.670631   \n",
       "2  0.000000  0.000000  1.000000  0.000000  0.000000  0.791821  0.000000   \n",
       "3  0.000000  0.000000  0.000000  1.000000  0.506866  0.000000  0.000000   \n",
       "4  0.000000  0.225489  0.000000  0.506866  1.000000  0.000000  0.000000   \n",
       "5  0.192353  0.157845  0.791821  0.000000  0.000000  1.000000  0.115488   \n",
       "6  0.817246  0.670631  0.000000  0.000000  0.000000  0.115488  1.000000   \n",
       "7  0.000000  0.000000  0.850516  0.000000  0.000000  0.930989  0.000000   \n",
       "\n",
       "          7  \n",
       "0  0.000000  \n",
       "1  0.000000  \n",
       "2  0.850516  \n",
       "3  0.000000  \n",
       "4  0.000000  \n",
       "5  0.930989  \n",
       "6  0.000000  \n",
       "7  1.000000  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarity_matrix = cosine_similarity(tv_matrix)\n",
    "similarity_df = pd.DataFrame(similarity_matrix)\n",
    "similarity_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e3ee4a",
   "metadata": {},
   "source": [
    "## Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc6bb2c",
   "metadata": {},
   "source": [
    "Humans have always excelled at understanding languages. It is easy for humans to understand the relationship between words but for computers, this task may not be simple. Word embeddings are basically a form of word representation that bridges the human understanding of language to that of a machine. For example, we humans understand the words like king and queen, man and woman, tiger and tigress have a certain type of relation between them but how can a computer figure this out?\n",
    "\n",
    "The different encoding we have discussed so far is arbitrary as it does not capture any relationship between words. \n",
    "It can be challenging for a model to interpret, for example, a linear classifier learns a single weight for each feature. \n",
    "Because there is no relationship between the similarity of any two words and the similarity of their encodings, this feature-weight combination is not meaningful.\n",
    "\n",
    "It should be nice to have representations of text in an n-dimensional space where words that have the same meaning have a similar representation. Meaning that two similar words are represented by almost similar vectors that are very closely placed in a vector space. \n",
    "\n",
    "Thus when using word embeddings, all individual words are represented as real-valued vectors in a predefined vector space. \n",
    "Each word is mapped to one vector and the vector values are learned in a way that resembles a neural network. Word2Vec is one of the most popular technique to learn word embeddings using shallow neural network. It was developed by Tomas Mikolov in 2013 at Google.\n",
    "\n",
    "The concept of embeddings arises from a branch of Natural Language Processing called - “Distributional Semantics”. It is based on the simple intuition that: ***Words that occur in similar contexts tend to have similar meanings***. In other words, a word’s meaning is given by the words that it appears frequently with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b9d50b",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92315c13",
   "metadata": {},
   "source": [
    "Word2vec is a method to efficiently create word embeddings by using a two-layer neural network.  It was developed by Tomas Mikolov, et al. at Google in 2013 as a response to make the neural-network-based training of the embedding more efficient and since then has become the de facto standard for developing pre-trained word embedding. \n",
    "\n",
    "The input of word2vec is a text corpus and its output is a set of vectors known as feature vectors that represent words in that corpus. The Word2Vec objective function causes the words that have a similar context to have similar embeddings. \n",
    "Thus in this vector space, these words are really close. Mathematically, the cosine of the angle (Q) between such vectors should be close to 1, i.e. angle close to 0.\n",
    "\n",
    "Word2vec is not a single algorithm but a combination of two techniques - **CBOW(Continuous bag of words)** and **Skip-gram** model. Both these are shallow neural networks which map word(s) to the target variable which is also a word(s). Both these techniques learn weights which act as word vector representations. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d899ea0",
   "metadata": {},
   "source": [
    "### CBOW Architecture \n",
    "\n",
    "CBOW predicts the probability of a word to occur **given the words surrounding it**. We can consider a single word or a group of words. But for simplicity, we will take a single context word and try to predict a single target word.\n",
    "The English language contains almost 1.2 million words, making it impossible to include so many words in our example. So we will consider a small example in which we have only four words i.e. ***live***, ***home***, ***they*** and ***at***. For simplicity, we will consider that the corpus contains only one sentence, that being, ***They live at home***. First, we convert each word into a one-hot encoding form. Also, we'll not consider all the words in the sentence but ll only take certain words that are in a window."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680f6605",
   "metadata": {},
   "source": [
    "![image.png](./pic/word-embedding-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8d9dea",
   "metadata": {},
   "source": [
    "For example for a window size equal to three, we only consider three words\n",
    "in a sentence. The middle word is to be predicted and the surrounding two\n",
    "words are fed into the neural network as context. The window is then slid\n",
    "and the process is repeated again. Finally, after training the network repeatedly by sliding the window a\n",
    "shown above, we get weights which we use to get the embeddings as\n",
    "shown below. Usually, we take a window size of around 8-10 words and have a\n",
    "vector size of 300."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b585ff",
   "metadata": {},
   "source": [
    "![image.png](./pic/word-embedding-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c585926",
   "metadata": {},
   "source": [
    "### Skip-gram model\n",
    "\n",
    "The Skip-gram model architecture usually tries to achieve the reverse of what the CBOW model does. It tries to predict the source context words (surrounding words) given a target word (the centre word)\n",
    "\n",
    "The working of the skip-gram model is quite similar to the CBOW but there is just a difference in the architecture of its neural network and the way the weight matrix is generated  as shown in the figure below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651bac1d",
   "metadata": {},
   "source": [
    "![image.png](./pic/3_1_text_vectorization_pic_16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a41294b",
   "metadata": {},
   "source": [
    "So now which one of the two algorithms should we use for implementing word2vec? Turns out for large corpus with higher dimensions, it is better to use skip-gram but is slow to train. Whereas CBOW is better for small corpus and is faster to train too."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dd92e5",
   "metadata": {},
   "source": [
    "## Implementing a word2vec model using a CBOW NN architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edfc1f4",
   "metadata": {},
   "source": [
    "### Load up sample corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "254a8611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "19f28544",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "book = gutenberg.sents('carroll-alice.txt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8e5a7e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines: 1703\n",
      "\n",
      "Sample line: ['The', 'rabbit', '-', 'hole', 'went', 'straight', 'on', 'like', 'a', 'tunnel', 'for', 'some', 'way', ',', 'and', 'then', 'dipped', 'suddenly', 'down', ',', 'so', 'suddenly', 'that', 'Alice', 'had', 'not', 'a', 'moment', 'to', 'think', 'about', 'stopping', 'herself', 'before', 'she', 'found', 'herself', 'falling', 'down', 'a', 'very', 'deep', 'well', '.']\n",
      "Total lines: 1290\n",
      "\n",
      "Processed line: thought alice fall shall think nothing tumbling stairs\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "\n",
    "remove_terms = punctuation + '0123456789'\n",
    "\n",
    "norm_book = [[word.lower() for word in sent if word not in remove_terms] for sent in book]\n",
    "norm_book = [' '.join(tok_sent) for tok_sent in norm_book]\n",
    "norm_book = filter(None, normalize_corpus(norm_book))\n",
    "norm_book = [tok_sent for tok_sent in norm_book if len(tok_sent.split()) > 2]\n",
    "\n",
    "print('Total lines:', len(book))\n",
    "print('\\nSample line:', book[10])\n",
    "\n",
    "print('Total lines:', len(norm_book))\n",
    "print('\\nProcessed line:', norm_book[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f458522d",
   "metadata": {},
   "source": [
    "### Build Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e273b4",
   "metadata": {},
   "source": [
    "**What is Padding***\n",
    "\n",
    "Padding comes from the need to encode sequence data into contiguous batches: in order to make all sequences in a batch fit a given standard length, it is necessary to pad or truncate some sequences.\n",
    "\n",
    "Let's take a close look.\n",
    "\n",
    "When processing sequence data, it is very common for individual samples to have different lengths. Consider the following example (text tokenized as words):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d2bcb253",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = [\n",
    "  [\"Hello\", \"world\", \"!\"],\n",
    "  [\"How\", \"are\", \"you\", \"doing\", \"today\"],\n",
    "  [\"The\", \"weather\", \"will\", \"be\", \"nice\", \"tomorrow\"],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead5a364",
   "metadata": {},
   "source": [
    "After vocabulary lookup, the data might be vectorized as integers, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7b79a2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = [\n",
    "  [71, 1331, 4231],\n",
    "  [73, 8, 3215, 55, 927], \n",
    "  [83, 91, 1, 645, 1253, 927],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96481487",
   "metadata": {},
   "source": [
    "The data is a nested list where individual samples have length 3, 5, and 6, respectively. **Since the input data for a deep learning model must be a single tensor** (of shape e.g. (batch_size, 6, vocab_size) in this case), **samples that are shorter than the longest item need to be padded with some placeholder value** (alternatively, one might also truncate long samples before padding short samples).\n",
    "\n",
    "Keras provides a utility function to truncate and pad Python lists to a common length: `tf.keras.preprocessing.sequence.pad_sequences`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2ed114f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 711  632   71    0    0    0]\n",
      " [  73    8 3215   55  927    0]\n",
      " [  83   91    1  645 1253  927]]\n"
     ]
    }
   ],
   "source": [
    "import keras \n",
    "\n",
    "raw_inputs = [\n",
    "    [711, 632, 71],\n",
    "    [73, 8, 3215, 55, 927],\n",
    "    [83, 91, 1, 645, 1253, 927],\n",
    "]\n",
    "\n",
    "# By default, this will pad using 0s; it is configurable via the\n",
    "# \"value\" parameter.\n",
    "# Note that you could \"pre\" padding (at the beginning) or\n",
    "# \"post\" padding (at the end).\n",
    "# We recommend using \"post\" padding when working with RNN layers\n",
    "# (in order to be able to use the\n",
    "# CuDNN implementation of the layers).\n",
    "padded_inputs = keras.preprocessing.sequence.pad_sequences(\n",
    "    raw_inputs, padding=\"post\"\n",
    ")\n",
    "print(padded_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d3d23d",
   "metadata": {},
   "source": [
    "`fit_on_texts` Updates internal vocabulary based on a list of texts. This method creates the vocabulary index based on word frequency. So if you give it something like, \"The cat sat on the mat.\" It will create a dictionary s.t. word_index[\"the\"] = 1; word_index[\"cat\"] = 2 it is word -> index dictionary so every word gets a unique integer value. 0 is reserved for padding. So ***lower integer means more frequent word*** (often the first few are stop words because they appear a lot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "458ac174",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import text\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils         import np_utils\n",
    "\n",
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(norm_book)\n",
    "word2id = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c6f83d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# in this case we explicity indicate that the placeholder to be used for padding is 0 (this is not strictly necessary)\n",
    "#\n",
    "word2id['PAD'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edfe6a9",
   "metadata": {},
   "source": [
    "`texts_to_word_sequences` Transforms each text in a sequence of word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "03b15227",
   "metadata": {},
   "outputs": [],
   "source": [
    "wids = [[word2id[w] for w in text.text_to_word_sequence(row)] for row in norm_book]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4910bd79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 337, 669, 1273, 1274]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wids[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42f487f",
   "metadata": {},
   "source": [
    "We can make also a very simple mapping to reverse the integer coding and recover the word from the id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "177cd987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 said\n",
      "2 alice\n",
      "3 little\n",
      "4 one\n",
      "5 would\n",
      "6 know\n",
      "7 went\n",
      "8 like\n",
      "9 could\n"
     ]
    }
   ],
   "source": [
    "id2word = {v:k for k, v in word2id.items()}\n",
    "for i in range(1,10):\n",
    "    print(i, id2word[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "364b4bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alice\n",
      "adventures\n",
      "wonderland\n",
      "lewis\n",
      "carroll\n"
     ]
    }
   ],
   "source": [
    "for w in wids[0]:\n",
    "    print(id2word[w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fc978e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 2379\n",
      "Vocabulary Sample: [('said', 1), ('alice', 2), ('little', 3), ('one', 4), ('would', 5), ('know', 6), ('went', 7), ('like', 8), ('could', 9), ('time', 10)]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(word2id)\n",
    "embed_size = 100\n",
    "window_size = 2\n",
    "\n",
    "print('Vocabulary Size:', vocab_size)\n",
    "print('Vocabulary Sample:', list(word2id.items())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e696ca8",
   "metadata": {},
   "source": [
    "### Build (context_words, target_word) pair generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6ed016ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_context_word_pairs(corpus, window_size, vocab_size):\n",
    "    context_length = window_size*2\n",
    "    for words in corpus:\n",
    "        sentence_length = len(words)\n",
    "        for index, word in enumerate(words):\n",
    "            context_words = []\n",
    "            label_word    = []            \n",
    "            start         = index - window_size\n",
    "            end           = index + window_size + 1\n",
    "            \n",
    "            context_words.append([words[i] \n",
    "                                 for i in range(start, end) \n",
    "                                 if 0 <= i < sentence_length \n",
    "                                 and i != index])\n",
    "            label_word.append(word)\n",
    "\n",
    "            x = sequence.pad_sequences(context_words, maxlen=context_length)\n",
    "            y = np_utils.to_categorical(label_word, vocab_size)\n",
    "            yield (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ec91befb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context (X): ['alice', 'adventures', 'lewis', 'carroll'] -> Target (Y): wonderland\n",
      "Context (X): ['alice', 'beginning', 'tired', 'sitting'] -> Target (Y): get\n",
      "Context (X): ['beginning', 'get', 'sitting', 'sister'] -> Target (Y): tired\n",
      "Context (X): ['get', 'tired', 'sister', 'bank'] -> Target (Y): sitting\n",
      "Context (X): ['tired', 'sitting', 'bank', 'nothing'] -> Target (Y): sister\n",
      "Context (X): ['sitting', 'sister', 'nothing', 'twice'] -> Target (Y): bank\n",
      "Context (X): ['sister', 'bank', 'twice', 'peeped'] -> Target (Y): nothing\n",
      "Context (X): ['bank', 'nothing', 'peeped', 'book'] -> Target (Y): twice\n",
      "Context (X): ['nothing', 'twice', 'book', 'sister'] -> Target (Y): peeped\n",
      "Context (X): ['twice', 'peeped', 'sister', 'reading'] -> Target (Y): book\n",
      "Context (X): ['peeped', 'book', 'reading', 'pictures'] -> Target (Y): sister\n"
     ]
    }
   ],
   "source": [
    "# Rember that argwhere find the indices of array elements that are non-zero, grouped by element.\n",
    "i = 0\n",
    "for x, y in generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size):\n",
    "    if 0 not in x[0]:\n",
    "        print('Context (X):', [id2word[w] for w in x[0]], '-> Target (Y):', id2word[np.argwhere(y[0])[0][0]])\n",
    "    \n",
    "        if i == 10:\n",
    "            break\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b2e414",
   "metadata": {},
   "source": [
    "### Build CBOW Deep Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5c02b6",
   "metadata": {},
   "source": [
    "**What is a \"backend\"?**\n",
    "\n",
    "Keras is a model-level library, providing high-level building blocks for developing deep learning models. It does not handle itself low-level operations such as tensor products, convolutions and so on. Instead, it relies on a specialized, well-optimized tensor manipulation library to do so, serving as the \"backend engine\" of Keras. Rather than picking one single tensor library and making the implementation of Keras tied to that library, Keras handles the problem in a modular way, and several different backend engines can be plugged seamlessly into Keras. At this time, Keras has two backend implementations available: the TensorFlow backend and the Theano backend.\n",
    "\n",
    "\n",
    "**The Embedding Layer**\n",
    "\n",
    "The Embedding layer is defined as the first hidden layer of a network. It must specify 3 arguments:\n",
    "\n",
    "- **input_dim**: This is the size of the vocabulary in the text data. For example, if your data is integer encoded to values between 0-10, then the size of the vocabulary would be 11 words.\n",
    "\n",
    "- **output_dim**: This is the size of the vector space in which words will be embedded. It defines the size of the output vectors from this layer for each word. For example, it could be 32 or 100 or even larger. Test different values for your problem.\n",
    "\n",
    "- **input_length**: This is the length of input sequences, as you would define for any input layer of a Keras model. For example, if all of your input documents are comprised of 1000 words, this would be 1000.\n",
    "\n",
    "**Lambda Layer**\n",
    "\n",
    "The Lambda layer exists so that arbitrary expressions can be used as a Layer when constructing Sequential and Functional API models. Lambda layers are best suited for simple operations or quick experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7682dead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 4, 100)            237900    \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2379)              240279    \n",
      "=================================================================\n",
      "Total params: 478,179\n",
      "Trainable params: 478,179\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Lambda\n",
    "\n",
    "cbow = Sequential()\n",
    "cbow.add(Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=window_size*2))\n",
    "cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(embed_size,)))\n",
    "cbow.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "cbow.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "print(cbow.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255d0b62",
   "metadata": {},
   "source": [
    "### Train model for 5 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7ca7ca",
   "metadata": {},
   "source": [
    "**What is `train_on_batch`**\n",
    "\n",
    "`train_on_batch` allows you to expressly update weights based on a collection of samples you provide, without regard to any fixed batch size. You would use this in cases when that is what you want: to train on an explicit collection of samples. You could use that approach to maintain your own iteration over multiple batches of a traditional training set but allowing fit or fit_generator to iterate batches for you is likely simpler.\n",
    "\n",
    "One case when it might be nice to use `train_on_batch` is for updating a pre-trained model on a single new batch of samples. Suppose you've already trained and deployed a model, and sometime later you've received a new set of training samples previously never used. You could use `train_on_batch` to directly update the existing model only on those samples. Other methods can do this too, but it is rather explicit to use train_on_batch for this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d3decc4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10000 (context, word) pairs\n",
      "Epoch: 1 \tLoss: 98396.26991511881\n",
      "\n",
      "Processed 10000 (context, word) pairs\n",
      "Epoch: 2 \tLoss: 98768.56974580139\n",
      "\n",
      "Processed 10000 (context, word) pairs\n",
      "Epoch: 3 \tLoss: 98866.98952811956\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5848\\1121201706.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgenerate_context_word_pairs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mi\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mcbow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m10000\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Processed {} (context, word) pairs'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[0;32m   1723\u001b[0m       iterator = data_adapter.single_batch_iterator(self.distribute_strategy, x,\n\u001b[0;32m   1724\u001b[0m                                                     \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1725\u001b[1;33m                                                     class_weight)\n\u001b[0m\u001b[0;32m   1726\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1727\u001b[0m       \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36msingle_batch_iterator\u001b[1;34m(strategy, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_make_class_weight_map_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1517\u001b[0m   \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_distribute_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1518\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1519\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1520\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    420\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 422\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    423\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[0;32m    680\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcomponents\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0melement_spec\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    681\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 682\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    683\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    684\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_create_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    684\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_create_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    685\u001b[0m     \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply_options\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m     \u001b[1;31m# Store dataset reference to ensure that dataset is alive when this iterator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m_apply_options\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    397\u001b[0m       dataset = _OptimizeDataset(dataset, graph_rewrites.enabled,\n\u001b[0;32m    398\u001b[0m                                  \u001b[0mgraph_rewrites\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisabled\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 399\u001b[1;33m                                  graph_rewrites.default, graph_rewrite_configs)\n\u001b[0m\u001b[0;32m    400\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m     \u001b[1;31m# (4) Apply stats aggregator options\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_dataset, optimizations_enabled, optimizations_disabled, optimizations_default, optimization_configs)\u001b[0m\n\u001b[0;32m   4581\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_optimizations_default\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4582\u001b[0m         \u001b[0moptimization_configs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimization_configs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4583\u001b[1;33m         **self._flat_structure)\n\u001b[0m\u001b[0;32m   4584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4585\u001b[0m     \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_OptimizeDataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariant_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\u001b[0m in \u001b[0;36moptimize_dataset_v2\u001b[1;34m(input_dataset, optimizations_enabled, optimizations_disabled, optimizations_default, output_types, output_shapes, optimization_configs, name)\u001b[0m\n\u001b[0;32m   4022\u001b[0m         \u001b[0moptimizations_disabled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizations_default\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"output_types\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4023\u001b[0m         \u001b[0moutput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"output_shapes\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_shapes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"optimization_configs\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4024\u001b[1;33m         optimization_configs)\n\u001b[0m\u001b[0;32m   4025\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4026\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 15):\n",
    "    loss = 0.\n",
    "    i = 0\n",
    "    for x, y in generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size):\n",
    "        i += 1\n",
    "        loss += cbow.train_on_batch(x, y)\n",
    "        if i % 10000 == 0:\n",
    "            print('Processed {} (context, word) pairs'.format(i))\n",
    "\n",
    "    print('Epoch:', epoch, '\\tLoss:', loss)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08842eb1",
   "metadata": {},
   "source": [
    "### Get word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8da56e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2378, 100)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>alice</th>\n",
       "      <td>0.351183</td>\n",
       "      <td>0.041107</td>\n",
       "      <td>0.090196</td>\n",
       "      <td>-0.215083</td>\n",
       "      <td>0.170995</td>\n",
       "      <td>-0.345772</td>\n",
       "      <td>-0.119684</td>\n",
       "      <td>0.068980</td>\n",
       "      <td>-0.180764</td>\n",
       "      <td>0.051785</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.233945</td>\n",
       "      <td>0.513039</td>\n",
       "      <td>0.169514</td>\n",
       "      <td>0.510741</td>\n",
       "      <td>-0.673383</td>\n",
       "      <td>-0.098482</td>\n",
       "      <td>-0.123255</td>\n",
       "      <td>0.428761</td>\n",
       "      <td>0.035849</td>\n",
       "      <td>0.300336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>little</th>\n",
       "      <td>-0.188662</td>\n",
       "      <td>0.003105</td>\n",
       "      <td>-0.108142</td>\n",
       "      <td>-0.774432</td>\n",
       "      <td>-0.117824</td>\n",
       "      <td>0.101573</td>\n",
       "      <td>0.407862</td>\n",
       "      <td>0.025299</td>\n",
       "      <td>0.266874</td>\n",
       "      <td>-0.006119</td>\n",
       "      <td>...</td>\n",
       "      <td>0.181358</td>\n",
       "      <td>-0.111602</td>\n",
       "      <td>0.065270</td>\n",
       "      <td>-0.277129</td>\n",
       "      <td>0.226172</td>\n",
       "      <td>-0.273504</td>\n",
       "      <td>0.074520</td>\n",
       "      <td>-0.157811</td>\n",
       "      <td>-0.151565</td>\n",
       "      <td>-0.057601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>0.666672</td>\n",
       "      <td>-0.358681</td>\n",
       "      <td>0.191382</td>\n",
       "      <td>-0.675251</td>\n",
       "      <td>-0.274691</td>\n",
       "      <td>-0.153004</td>\n",
       "      <td>0.089441</td>\n",
       "      <td>-0.514175</td>\n",
       "      <td>0.083103</td>\n",
       "      <td>0.226477</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024816</td>\n",
       "      <td>0.182160</td>\n",
       "      <td>0.162045</td>\n",
       "      <td>0.405100</td>\n",
       "      <td>-0.020502</td>\n",
       "      <td>0.038454</td>\n",
       "      <td>0.313101</td>\n",
       "      <td>0.192715</td>\n",
       "      <td>-0.285474</td>\n",
       "      <td>0.370692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>would</th>\n",
       "      <td>0.321406</td>\n",
       "      <td>0.312559</td>\n",
       "      <td>0.088120</td>\n",
       "      <td>-0.499484</td>\n",
       "      <td>-0.450157</td>\n",
       "      <td>-0.186578</td>\n",
       "      <td>-0.084536</td>\n",
       "      <td>-0.374199</td>\n",
       "      <td>-0.205349</td>\n",
       "      <td>0.082250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083769</td>\n",
       "      <td>-0.178289</td>\n",
       "      <td>0.173287</td>\n",
       "      <td>-0.119762</td>\n",
       "      <td>0.374727</td>\n",
       "      <td>0.103557</td>\n",
       "      <td>0.187335</td>\n",
       "      <td>-0.063804</td>\n",
       "      <td>0.030449</td>\n",
       "      <td>0.244675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>know</th>\n",
       "      <td>0.036590</td>\n",
       "      <td>0.041059</td>\n",
       "      <td>0.210075</td>\n",
       "      <td>-0.268672</td>\n",
       "      <td>-0.176965</td>\n",
       "      <td>0.097156</td>\n",
       "      <td>-0.027579</td>\n",
       "      <td>-0.100951</td>\n",
       "      <td>-0.091927</td>\n",
       "      <td>-0.160180</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.145163</td>\n",
       "      <td>0.241729</td>\n",
       "      <td>-0.281267</td>\n",
       "      <td>0.135270</td>\n",
       "      <td>0.204009</td>\n",
       "      <td>-0.048631</td>\n",
       "      <td>0.183325</td>\n",
       "      <td>-0.234910</td>\n",
       "      <td>0.003380</td>\n",
       "      <td>-0.168523</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6   \\\n",
       "alice   0.351183  0.041107  0.090196 -0.215083  0.170995 -0.345772 -0.119684   \n",
       "little -0.188662  0.003105 -0.108142 -0.774432 -0.117824  0.101573  0.407862   \n",
       "one     0.666672 -0.358681  0.191382 -0.675251 -0.274691 -0.153004  0.089441   \n",
       "would   0.321406  0.312559  0.088120 -0.499484 -0.450157 -0.186578 -0.084536   \n",
       "know    0.036590  0.041059  0.210075 -0.268672 -0.176965  0.097156 -0.027579   \n",
       "\n",
       "              7         8         9   ...        90        91        92  \\\n",
       "alice   0.068980 -0.180764  0.051785  ... -0.233945  0.513039  0.169514   \n",
       "little  0.025299  0.266874 -0.006119  ...  0.181358 -0.111602  0.065270   \n",
       "one    -0.514175  0.083103  0.226477  ... -0.024816  0.182160  0.162045   \n",
       "would  -0.374199 -0.205349  0.082250  ...  0.083769 -0.178289  0.173287   \n",
       "know   -0.100951 -0.091927 -0.160180  ... -0.145163  0.241729 -0.281267   \n",
       "\n",
       "              93        94        95        96        97        98        99  \n",
       "alice   0.510741 -0.673383 -0.098482 -0.123255  0.428761  0.035849  0.300336  \n",
       "little -0.277129  0.226172 -0.273504  0.074520 -0.157811 -0.151565 -0.057601  \n",
       "one     0.405100 -0.020502  0.038454  0.313101  0.192715 -0.285474  0.370692  \n",
       "would  -0.119762  0.374727  0.103557  0.187335 -0.063804  0.030449  0.244675  \n",
       "know    0.135270  0.204009 -0.048631  0.183325 -0.234910  0.003380 -0.168523  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = cbow.get_weights()[0]\n",
    "weights = weights[1:]\n",
    "print(weights.shape)\n",
    "\n",
    "pd.DataFrame(weights, index=list(id2word.values())[1:]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea34dd72",
   "metadata": {},
   "source": [
    "### Build a distance matrix to view the most similar words (contextually)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cf1b07f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2378, 2378)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cat': ['cheshire', 'mad', 'pig', 'pointing', 'sneeze'],\n",
       " 'caterpillar': ['hookah', 'exactly', 'mushroom', 'held', 'puppy'],\n",
       " 'hatter': ['bread', 'butter', 'officers', 'dormouse', 'write'],\n",
       " 'queen': ['three', 'behind', 'hedgehog', 'duchess', 'spoke'],\n",
       " 'flamingo': ['fight', 'executions', 'music', 'roses', 'drew']}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "# compute pairwise distance matrix\n",
    "distance_matrix = euclidean_distances(weights)\n",
    "print(distance_matrix.shape)\n",
    "\n",
    "# view contextually similar words\n",
    "similar_words = {search_term: [id2word[idx] for idx in distance_matrix[word2id[search_term]-1].argsort()[1:6]+1] \n",
    "                   for search_term in ['cat', 'caterpillar', 'hatter', 'queen']}\n",
    "\n",
    "similar_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1707db2",
   "metadata": {},
   "source": [
    "## Pre-Trained Embedding Models\n",
    "\n",
    "Although in real applications we train our model over Wikipedia text with a window size around 5- 10. \n",
    "The number of words in the corpus is around 13 million, hence it takes a huge amount of time and resources to generate these embeddings. \n",
    "To avoid this we can use the pre-trained word vectors that are already trained and we can easily use them. \n",
    "Here are the links to download pre-trained Word2Vec or GloVe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53693ce4",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dbd976",
   "metadata": {},
   "source": [
    "## What is the meaning of the yield instruction in Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12cf1ac",
   "metadata": {},
   "source": [
    "To understand what yield does, you must understand what generators are. And before you can understand generators, you must understand iterables.\n",
    "\n",
    "**Iterables**\n",
    "\n",
    "When you create a list, you can read its items one by one. Reading its items one by one is called iteration. `mylist` is an iterable. When you use a list comprehension, you create a list, and so an iterable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbb8abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mylist = [x*x for x in range(3)]\n",
    "for i in mylist:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650d2c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in mylist:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e495595",
   "metadata": {},
   "source": [
    "Everything you can use \"for... in...\" on is an iterable; lists, strings, files...\n",
    "\n",
    "These iterables are handy because you can read them as much as you wish, but **you store all the values in memory** and this is not always what you want when you have a lot of values.\n",
    "\n",
    "**Generators**\n",
    "\n",
    "Generators are iterators, a kind of iterable you can only iterate over once. Generators **do not store all the values in memory, they generate the values on the fly**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0f1007",
   "metadata": {},
   "outputs": [],
   "source": [
    "mygenerator = (x*x for x in range(3))\n",
    "for i in mygenerator:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb611032",
   "metadata": {},
   "source": [
    "It is just the same except you used () instead of []. BUT, you cannot perform `for i in mygenerator` a second time since generators can only be used once: they calculate 0, then forget about it and calculate 1, and end calculating 4, one by one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d17a008",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in mygenerator:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363cc067",
   "metadata": {},
   "source": [
    "**Yield**\n",
    "\n",
    "yield is a keyword that is used like return, except ***the function will return a generator***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51aaeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_generator():\n",
    "    mylist = range(10)\n",
    "    for i in mylist:\n",
    "        yield i*i\n",
    "\n",
    "mygenerator = create_generator() # create a generator\n",
    "print(mygenerator) # mygenerator is an object!\n",
    "\n",
    "for i in mygenerator:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19d6848",
   "metadata": {},
   "source": [
    "# References and Credits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4e7ef5",
   "metadata": {},
   "source": [
    "***Bird S. et al.***, \"*Natural Language Processing with Python*\" O'Reilly (2009)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c93f4c",
   "metadata": {},
   "source": [
    "***Bengfort B. et al.***, \"*Applied Text Analysis with Python*\" O'Reilly (2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e440b6f5",
   "metadata": {},
   "source": [
    "***Sarkar D.***, \"*Text Analytics with Python A Practitioner's Guide to Natural Language Processing 2nd Ed*\" APress (2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f78b09",
   "metadata": {},
   "source": [
    "***Pennington J. et al.***, \"*GloVe: Global Vectors for Word Representation*\", [pdf](https://nlp.stanford.edu/pubs/glove.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd11c946",
   "metadata": {},
   "source": [
    "***Mikolov T. et al.***, \"*Efficient Estimation of Word Representations in Vector Space*\", arXiv:1301.3781v3  7 Sep 2013 [pdf](https://arxiv.org/abs/1301.3781)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": "4",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
