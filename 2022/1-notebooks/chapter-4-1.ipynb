{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01435e96",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/polyhedron-gdl/advanced-machine-learning-for-finance/blob/main/2022/1-notebooks/chapter-4-1.ipynb\">\n",
    "        <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9ff4b8",
   "metadata": {},
   "source": [
    "# Natural Language Processing: Tools and Ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621aee4a",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Natural language processing (NLP) is a field of machine learning in which computers analyze, understand, and derive meaning from human language in a smart and useful way. By utilizing NLP, developers can organize and structure knowledge to perform tasks such as automatic summarization, translation, named entity recognition, relationship extraction, sentiment analysis, speech recognition, and topic segmentation.\n",
    "\n",
    "NLP is characterized as a difficult problem in computer science. Human language is rarely precise, or plainly spoken. To understand human language is to understand not only the words, but the concepts and how they’re linked together to create meaning. Despite language being one of the easiest things for the human mind to learn, the ambiguity of language is what makes natural language processing a difficult problem for computers to master.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f60b82d",
   "metadata": {},
   "source": [
    "## The NLTK package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ae37a5",
   "metadata": {},
   "source": [
    "What is NLTK? The [Natural Language Toolkit](https://www.nltk.org/), or more commonly NLTK, is a suite of libraries and programs for symbolic and statistical natural language processing (NLP) for English written in the Python programming language. It was developed by Steven Bird and Edward Loper in the Department of Computer and Information Science at the University of Pennsylvania.NLTK includes graphical demonstrations and sample data. It is accompanied by a book that explains the underlying concepts behind the language processing tasks supported by the toolkit, plus a text book available also on line [here](https://www.nltk.org/book/)\n",
    "\n",
    "NLTK is intended to support research and teaching in NLP or closely related areas, including empirical linguistics, cognitive science, artificial intelligence, information retrieval, and machine learning.NLTK has been used successfully as a teaching tool, as an individual study tool, and as a platform for prototyping and building research systems. There are 32 universities in the US and 25 countries using NLTK in their courses. NLTK supports classification, tokenization, stemming, tagging, parsing, and semantic reasoning functionalities. \n",
    "\n",
    "The latest version is NLTK 3.3. It can be used by students, researchers, and industrialists. It is an Open Source and free library. It is available for Windows, Mac OS, and Linux. \n",
    "\n",
    "You can install nltk using pip installer if it is not installed in your Python installation. To test the installation:\n",
    "\n",
    "- Open your Python IDE or the CLI interface (whichever you use normally)\n",
    "- Type `import nltk` and press enter if no message of missing nltk is shown then nltk is installed on your computer.\n",
    "\n",
    "After installation, nltk also provides test datasets to work within Natural Language Processing. You can download it by using the following commands in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0744d8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43da2a82",
   "metadata": {},
   "source": [
    "### Using NLTK Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d08f133",
   "metadata": {},
   "source": [
    "You can use the NLTK Text Corpora which is a vast repository for a large body of text called as a Corpus which can be used while you are working with Natural Language Processing (NLP) with Python. There are many different types of corpora available that you can use with varying types of projects, for example, a selection of free electronic books, web and chat text and news documents on different genres.\n",
    "\n",
    "In the online book site you can find everything you need to known to access the NLTK Corpus, in particular you can start from [Chapter 2 - Accessing Text Corpora and Lexical Resources](https://www.nltk.org/book/ch02.html).\n",
    "\n",
    "Here is an example of how you can use a corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3972ee4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f977012",
   "metadata": {},
   "source": [
    "### Loading Your Own Corpus\n",
    "\n",
    "If you have your own collection of text files that you would like to access using the above methods, you can easily load them with the help of NLTK's `PlaintextCorpusReader`. Check the location of your files on your file system; in the following example, we have taken this to be the directory `C:\\Corpus\\EBA`. Whatever the location, set this to be the value of corpus_root.\n",
    "\n",
    "The second parameter of the PlaintextCorpusReader initializer can be a list of fileids, like ['a.txt', 'test/b.txt'], or a pattern that matches all fileids, like '[abc]/.*\\.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a1e5125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Final Guidelines on Accounting for Expected Credit Losses (EBA-GL-2017-06).txt',\n",
       " 'Final Guidelines on the management of interest rate risk arising from non-trading activities.txt',\n",
       " 'Final Report on Guidelines on LGD estimates under downturn conditions.txt',\n",
       " 'Final Report on Guidelines on default definition (EBA-GL-2016-07).txt',\n",
       " 'Final Report on Guidelines on uniform disclosure of IFRS9 transitional arrangements (EBA-GL-2018-01).txt',\n",
       " 'Final report on updated GL Funding Plans (EBA 9.12.2019).txt',\n",
       " 'Final report on updated GL Funding Plans_(EBA-GL-2019-05)_09122019.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "\n",
    "corpus_root = '.\\corpus\\EBA'\n",
    "corpus_list = PlaintextCorpusReader(corpus_root, '.*', encoding='latin-1')\n",
    "corpus_list.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c00b12",
   "metadata": {},
   "source": [
    "Let's pick out one of these text (for example the guideline on the interest rate), give it a short name, irrbb_w."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3346b349",
   "metadata": {},
   "outputs": [],
   "source": [
    "id = \"Final Guidelines on the management of interest rate risk arising from non-trading activities.txt\"\n",
    "irrbb_w = corpus_list.words(id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dbedb6",
   "metadata": {},
   "source": [
    "For more information about corpora readers see also *Bengfort B. et al. \"Applied Text Analysis with Python\" O'Reilly (2018) Chapter 2*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1d8829",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aceccef",
   "metadata": {},
   "source": [
    "In this section I want to go over some important NLP concepts and show code examples on how to apply them on text data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42647a50",
   "metadata": {},
   "source": [
    "Once you are sure that all documents loaded properly, go on to **preprocess your texts**.\n",
    "This step allows you to **remove numbers, dealing with capitalization, common words, punctuation, and otherwise prepare your texts for analysis**\n",
    "\n",
    "Data cleansing, though tedious, is perhaps the most important step in text analysis.   As we will see, dirty data can play havoc with the results.  Furthermore, as we will also see, data cleaning is invariably an iterative process as there are always problems that are overlooked the first time around."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3edc465",
   "metadata": {},
   "source": [
    "**Removing punctuation**:\n",
    "Your computer cannot actually read. Punctuation and other special characters only look like more words to your computer and Python. Use the following to methods to remove them from your text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c92a8e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Clear', 'and', 'effective', 'communication', 'is', 'very', 'important', 'to', 'Our', 'monetary', 'policy', 'becomes', 'more', 'effective', 'when', 'our', 'decisions', 'are', 'better', 'The', 'media', 'play', 'an', 'important', 'role', 'in', 'this', 'process', 'and', 'help', 'keep', 'us', 'accountable', 'to', 'the', 'European']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Clear and effective communication is very important to us. Our monetary policy becomes \\\n",
    "            more effective when our decisions are better understood. The media play an important \\\n",
    "            role in this process and help keep us accountable to the European public.\"\n",
    "tokens = sentence.split()\n",
    "# remove all tokens that are not alphabetic\n",
    "words = [word for word in tokens if word.isalpha()]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad3cdb0",
   "metadata": {},
   "source": [
    "Note that this result is not completely correct. Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eea5a6c",
   "metadata": {},
   "source": [
    "**Converting to lowercase**:\n",
    "As before, we want a word to appear exactly the same every time it appears. Since many languages **are** case sensitive, “Text” is not equal to “text” – and hence the rationale for converting to a standard case. We therefore change everything to lowercase. This is also a good time to check for any other special symbols that may need to be removed manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "695d5d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['clear', 'and', 'effective', 'communication', 'is', 'very', 'important', 'to', 'our', 'monetary', 'policy', 'becomes', 'more', 'effective', 'when', 'our', 'decisions', 'are', 'better', 'the', 'media', 'play', 'an', 'important', 'role', 'in', 'this', 'process', 'and', 'help', 'keep', 'us', 'accountable', 'to', 'the', 'european']\n"
     ]
    }
   ],
   "source": [
    "# all lowercase\n",
    "words = [word.lower() for word in words]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb38948b",
   "metadata": {},
   "source": [
    "**Removing numbers**: Text analysts are typically not interested in numbers since these do not usually contribute to the meaning of the text. **However, this may not always be so. For example, it is definitely not the case if one is interested in getting a count of the number of times a particular year appears in a corpus**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4be488b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove numbers\n",
    "words = [w for w in words if not w.isdigit()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89860dc",
   "metadata": {},
   "source": [
    "**Removing “Stop Words” (common words)** \n",
    "\n",
    "In every text, there are a lot of common, and uninteresting words that we generically call Stop WOrds. Stop words are words that may not carry any valuable information, like articles (“the”), conjunctions (“and”), or propositions (“with”). Why would you want to remove them? Because finding out that “the” and “a” are the most common words in your dataset doesn’t tell you much about the data. Such words are frequent by their nature, and will confound your analysis if they remain in the text. \n",
    "\n",
    "NLP Python libraries like NLTK usually come with an in-built stopword list which you can easily import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab375bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "example_sent = \"This is a sample sentence, showing off the stop words filtration.\"\n",
    "stop_words = set(stopwords.words('english'))\n",
    "word_tokens = word_tokenize(example_sent)\n",
    "\n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4367f9e",
   "metadata": {},
   "source": [
    "Note, that in some cases stop words matter. For example, in identifying negative reviews or recommendations. People will use stop words like “no” and “not” in negative reviews: “I will not buy this product again. I saw no benefits in using it”. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b340f498",
   "metadata": {},
   "source": [
    "**Removing particular words**:\n",
    "\n",
    "The NLTK stopword list, however, only has around 200 stopwords. The stopword list which one can commonly use for text analysis may contains almost 600 words. So if you find that a particular word or two appear in the output, but are not of value to your particular analysis, you can remove them, specifically, from the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cfa96f",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "Tokenization is a process of splitting a text object into smaller units which are also called tokens. Examples of tokens can be words, numbers, engrams, or even symbols. Single words are called unigrams, two words bi-grams, and three words tri-grams.\n",
    "\n",
    "The most commonly used tokenization process is White-space Tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc5eadcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Clear', 'and', 'effective', 'communication', 'is', 'very', 'important', 'to', 'us.', 'Our', 'monetary', 'policy', 'becomes', 'more', 'effective', 'when', 'our', 'decisions', 'are', 'better', 'understood.', 'The', 'media', 'play', 'an', 'important', 'role', 'in', 'this', 'process', 'and', 'help', 'keep', 'us', 'accountable', 'to', 'the', 'European', 'public.']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Clear and effective communication is very important to us. Our monetary policy becomes more \\\n",
    "            effective when our decisions are better understood. The media play an important role in this \\\n",
    "            process and help keep us accountable to the European public.\"\n",
    "tokens = sentence.split()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f960ec5",
   "metadata": {},
   "source": [
    "When dealing with a new dataset it's often helpful to extract the most common words to get an idea of what the data is about. You usually want to extract the most common unigrams first, but it can also be useful to extract n-grams with larger n to identify patterns. NLTK has in-built bigrams, trigrams and ngrams functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9066ab10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams, bigrams, trigrams\n",
    "\n",
    "sen = \"Clear and effective communication is very important to us. Our monetary policy becomes more \\\n",
    "            effective when our decisions are better understood. The media play an important role in this \\\n",
    "            process and help keep us accountable to the European public.\"\n",
    "nltk_tokens = word_tokenize(sen) #using tokenize from NLKT and not split() because split() does not take into account punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36256e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Clear', 'and', 'effective', 'communication', 'is', 'very', 'important', 'to', 'us', 'Our', 'monetary', 'policy', 'becomes', 'more', 'effective', 'when', 'our', 'decisions', 'are', 'better', 'understood', 'The', 'media', 'play', 'an', 'important', 'role', 'in', 'this', 'process', 'and', 'help', 'keep', 'us', 'accountable', 'to', 'the', 'European', 'public']\n"
     ]
    }
   ],
   "source": [
    "# remove all tokens that are not alphabetic\n",
    "words = [word for word in nltk_tokens if word.isalpha()]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3698aad2",
   "metadata": {},
   "source": [
    "This is better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a79a2fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Clear', 'and'), ('and', 'effective'), ('effective', 'communication'), ('communication', 'is'), ('is', 'very'), ('very', 'important'), ('important', 'to'), ('to', 'us'), ('us', '.'), ('.', 'Our'), ('Our', 'monetary'), ('monetary', 'policy'), ('policy', 'becomes'), ('becomes', 'more'), ('more', 'effective'), ('effective', 'when'), ('when', 'our'), ('our', 'decisions'), ('decisions', 'are'), ('are', 'better'), ('better', 'understood'), ('understood', '.'), ('.', 'The'), ('The', 'media'), ('media', 'play'), ('play', 'an'), ('an', 'important'), ('important', 'role'), ('role', 'in'), ('in', 'this'), ('this', 'process'), ('process', 'and'), ('and', 'help'), ('help', 'keep'), ('keep', 'us'), ('us', 'accountable'), ('accountable', 'to'), ('to', 'the'), ('the', 'European'), ('European', 'public'), ('public', '.')]\n",
      "[('Clear', 'and', 'effective'), ('and', 'effective', 'communication'), ('effective', 'communication', 'is'), ('communication', 'is', 'very'), ('is', 'very', 'important'), ('very', 'important', 'to'), ('important', 'to', 'us'), ('to', 'us', '.'), ('us', '.', 'Our'), ('.', 'Our', 'monetary'), ('Our', 'monetary', 'policy'), ('monetary', 'policy', 'becomes'), ('policy', 'becomes', 'more'), ('becomes', 'more', 'effective'), ('more', 'effective', 'when'), ('effective', 'when', 'our'), ('when', 'our', 'decisions'), ('our', 'decisions', 'are'), ('decisions', 'are', 'better'), ('are', 'better', 'understood'), ('better', 'understood', '.'), ('understood', '.', 'The'), ('.', 'The', 'media'), ('The', 'media', 'play'), ('media', 'play', 'an'), ('play', 'an', 'important'), ('an', 'important', 'role'), ('important', 'role', 'in'), ('role', 'in', 'this'), ('in', 'this', 'process'), ('this', 'process', 'and'), ('process', 'and', 'help'), ('and', 'help', 'keep'), ('help', 'keep', 'us'), ('keep', 'us', 'accountable'), ('us', 'accountable', 'to'), ('accountable', 'to', 'the'), ('to', 'the', 'European'), ('the', 'European', 'public'), ('European', 'public', '.')]\n"
     ]
    }
   ],
   "source": [
    "#splitting sentence into bigrams and trigrams\n",
    "print(list(bigrams(nltk_tokens)))\n",
    "print(list(trigrams(nltk_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d3c266",
   "metadata": {},
   "source": [
    "The sents() function divides the text up into sentencese, where each sentence is a list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ca460f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['GUIDELINES', 'ON', 'THE', 'MANAGEMENT', 'OF', 'INTEREST', 'RATE', 'RISK'], ['ARISING', 'FROM', 'NON', '-', 'TRADING', 'BOOK', 'ACTIVITIES'], ...]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irrbb_s = corpus_list.sents(id)\n",
    "irrbb_s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1ea1e0",
   "metadata": {},
   "source": [
    "## Lemmatisation and Stemming "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0d739b",
   "metadata": {},
   "source": [
    "Stemming and Lemmatization are Text Normalization (or sometimes called Word Normalization) techniques in the field of Natural Language Processing that are used to prepare text, words, and documents for further processing. Stemming and Lemmatization have been studied, and algorithms have been developed in Computer Science since the 1960's.\n",
    "\n",
    "Languages we speak and write are made up of several words often derived from one another. When a language contains words that are derived from another word as their use in the speech changes is called **Inflected Language**. [Inflection](https://en.wikipedia.org/wiki/Inflection) is the modification of a word to express different grammatical categories such as tense, case, voice, aspect, person, number, gender, and mood. An inflection expresses one or more grammatical categories with a prefix, suffix or infix, or another internal modification such as a vowel change.\n",
    "\n",
    "Typically a large corpus will contain many words that have a common root – for example: offer, offered and offering. Lemmatisation and stemming both refer to a process of reducing a word to its root. The difference is that stem might not be an actual word whereas, a lemma is an actual word. It’s a handy tool if you want to avoid treating different forms of the same word as different words. Let's consider the following example:\n",
    "\n",
    "- Lemmatising: considered, considering, consider → “consider”\n",
    "- Stemming: considered, considering, consider → “consid”\n",
    "\n",
    "Stemming and Lemmatization are widely used in tagging systems, indexing, SEOs, Web search results, and information retrieval. For example, searching for fish on Google will also result in fishes, fishing as fish is the stem of both words.\n",
    "\n",
    "NLTK comes with many different in-built lemmatisers and stemmers, so just plug and play."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5b175e",
   "metadata": {},
   "source": [
    "### NLTK Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af6303e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "consid\n",
      "considering\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer, PorterStemmer, SnowballStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "word = \"considering\"\n",
    "\n",
    "stemmed_word =  stemmer.stem(word)\n",
    "lemmatised_word = lemmatizer.lemmatize(word)\n",
    "\n",
    "print(stemmed_word)\n",
    "print(lemmatised_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489022c8",
   "metadata": {},
   "source": [
    "We can use any of the text of nltk corpora to make some test. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e345d34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file=nltk.corpus.gutenberg.words('melville-moby_dick.txt')\n",
    "my_lines_list=[]\n",
    "for line in text_file:\n",
    "    my_lines_list.append(line)\n",
    "#my_lines_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7fd31d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer=PorterStemmer()\n",
    "\n",
    "def stemSentence(sentence):\n",
    "    token_words=word_tokenize(sentence)\n",
    "    token_words\n",
    "    stem_sentence=[]\n",
    "    for word in token_words:\n",
    "        stem_sentence.append(stemmer.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18b5eaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_file=open(\"stemming_example.txt\",mode=\"a+\", encoding=\"utf-8\")\n",
    "\n",
    "for line in my_lines_list:\n",
    "    stem_sentence=stemSentence(line)\n",
    "    stem_file.write(stem_sentence)\n",
    "    \n",
    "stem_file.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3877fc11",
   "metadata": {},
   "source": [
    "Python nltk provides not only two English stemmers: PorterStemmer and LancasterStemmer but also a lot of non-English stemmers as part of SnowballStemmers, ISRIStemmer, RSLPSStemmer. Python NLTK included SnowballStemmers as a language to create to create non-English stemmers. One can program one's own language stemmer using snowball. Currently, it supports the following languages:\n",
    "- Danish\n",
    "- Dutch\n",
    "- English\n",
    "- French\n",
    "- German\n",
    "- Hungarian\n",
    "- Italian\n",
    "- Norwegian\n",
    "- Porter\n",
    "- Portuguese\n",
    "- Romanian\n",
    "- Russian\n",
    "- Spanish\n",
    "- Swedish\n",
    "\n",
    "ISRIStemmer is an Arabic stemmer and RSLPStemmer is stemmer for the Portuguese Language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b495aff3",
   "metadata": {},
   "source": [
    "### NLTK Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d098816",
   "metadata": {},
   "source": [
    "As we have already said, lemmatization, unlike Stemming, reduces the inflected words properly ensuring that the root word belongs to the language. In Lemmatization root word is called Lemma. A lemma (plural lemmas or lemmata) is the canonical form, dictionary form, or citation form of a set of words.\n",
    "\n",
    "For example, runs, running, ran are all forms of the word run, therefore run is the lemma of all these words. Because lemmatization returns an actual word of the language, it is used where it is necessary to get valid words.\n",
    "\n",
    "Python NLTK provides WordNet Lemmatizer that uses the WordNet Database to lookup lemmas of words. WordNet is a lexical database for the English language, which was created by Princeton, and is part of the NLTK corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29df92b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "He                  He                  \n",
      "was                 wa                  \n",
      "running             running             \n",
      "and                 and                 \n",
      "eating              eating              \n",
      "at                  at                  \n",
      "same                same                \n",
      "time                time                \n",
      "He                  He                  \n",
      "has                 ha                  \n",
      "bad                 bad                 \n",
      "habit               habit               \n",
      "of                  of                  \n",
      "swimming            swimming            \n",
      "after               after               \n",
      "playing             playing             \n",
      "long                long                \n",
      "hours               hour                \n",
      "in                  in                  \n",
      "the                 the                 \n",
      "Sun                 Sun                 \n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
    "punctuations=\"?:!.,;\"\n",
    "sentence_words = nltk.word_tokenize(sentence)\n",
    "for word in sentence_words:\n",
    "    if word in punctuations:\n",
    "        sentence_words.remove(word)\n",
    "\n",
    "sentence_words\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "for word in sentence_words:\n",
    "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1a8d55",
   "metadata": {},
   "source": [
    "In the above output, you must be wondering that no actual root form has been given for any word, this is because they are given without context. You need to provide the context in which you want to lemmatize that is the parts-of-speech (POS). This is done by giving the value for pos parameter in `wordnet_lemmatizer.lemmatize`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "45bff564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He                  He                  \n",
      "was                 be                  \n",
      "running             run                 \n",
      "and                 and                 \n",
      "eating              eat                 \n",
      "at                  at                  \n",
      "same                same                \n",
      "time                time                \n",
      "He                  He                  \n",
      "has                 have                \n",
      "bad                 bad                 \n",
      "habit               habit               \n",
      "of                  of                  \n",
      "swimming            swim                \n",
      "after               after               \n",
      "playing             play                \n",
      "long                long                \n",
      "hours               hours               \n",
      "in                  in                  \n",
      "the                 the                 \n",
      "Sun                 Sun                 \n"
     ]
    }
   ],
   "source": [
    "for word in sentence_words:\n",
    "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word, pos=\"v\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c816b92",
   "metadata": {},
   "source": [
    "### Stemming or lemmatization?\n",
    "\n",
    "After going through this section, you may be asking yourself when should I use Stemming and when should I use Lemmatization? We have seen the following points:\n",
    "\n",
    "Stemming and Lemmatization both generate the root form of the inflected words. The difference is that stem might not be an actual word whereas, lemma is an actual language word.\n",
    "\n",
    "Stemming follows an algorithm with steps to perform on the words which makes it faster. Whereas, in lemmatization, you used WordNet corpus and a corpus for stop words as well to produce lemma which makes it slower than stemming. You also had to define a parts-of-speech to obtain the correct lemma.\n",
    "\n",
    "So when to use what! The above points show that if speed is focused then stemming should be used since lemmatizers scan a corpus which consumed time and processing. It depends on the application you are working on that decides if stemmers should be used or lemmatizers. If you are building a language application in which language is important you should use lemmatization as it uses a corpus to match root forms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1983daca",
   "metadata": {},
   "source": [
    "## Part-of-Speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665b1f67",
   "metadata": {},
   "source": [
    "### What is POS Tagging?\n",
    "\n",
    "The process of classifying words into their parts of speech and labeling them accordingly is known as part-of-speech tagging or POS-tagging, or simply tagging. The part of speech **explains how a word is used in a sentence**. There are eight main parts of speech: \n",
    "\n",
    "- nouns, \n",
    "- pronouns, \n",
    "- adjectives, \n",
    "- verbs, \n",
    "- adverbs, \n",
    "- prepositions, \n",
    "- conjunctions\n",
    "- interjections.\n",
    "\n",
    "Examples:\n",
    "\n",
    " - **Noun (N)** : Daniel, London, table, dog, teacher, pen, city\n",
    " - **Verb (V)** : go, speak, run, eat, play, live, walk, have, like, are, is\n",
    " - **Adjective (ADJ)** : big, happy, green, young, fun, crazy, three\n",
    " - **Adverb (ADV)** : slowly, quietly, very, always, never, too, well, tomorrow\n",
    " - **Preposition (P)** : at, on, in, from, with, near, between, about, under\n",
    " - **Conjunction (CON)** : and, or, but, because, so, yet, unless, since, if\n",
    " - **Pronoun (PRO)** : I, you, we, they, he, she, it, me, us, them, him, her, this\n",
    " - **Interjection (INT)** : Ouch! Wow! Great! Help! Oh! Hey! Hi!\n",
    "\n",
    "Most POS are divided into sub-classes. POS Tagging simply means labeling words with their appropriate Part-Of-Speech.\n",
    "\n",
    "The collection of tags used for a particular task is known as a **Tagset**.\n",
    "\n",
    "A part-of-speech tagger, or POS-tagger, processes a sequence of words, and attaches a part of speech tag to each word.\n",
    "Lets first run the below code and see what exactly are we talking about."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba4040b",
   "metadata": {},
   "source": [
    "POS tagging is a **supervised learning solution** that uses features like the previous word, next word, is first letter capitalized etc. NLTK has a function to get pos tags and it works after tokenization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6590468a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My', 'name', 'is', 'Giovanni']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"My name is Giovanni\"\n",
    "token = nltk.word_tokenize(sentence)\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77ed8e3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('My', 'PRP$'), ('name', 'NN'), ('is', 'VBZ'), ('Giovanni', 'NNP')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5496c72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n"
     ]
    }
   ],
   "source": [
    "# We can get more details about any POS tag using help funciton of NLTK as follows.\n",
    "nltk.help.upenn_tagset(\"PRP$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "35446d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset(\"NN$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ae9a5dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset(\"VBZ$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e143223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset(\"NNP$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5555019",
   "metadata": {},
   "source": [
    "The most popular tag set is Penn Treebank tagset. Most of the already trained taggers for English are trained on this tag set. To view the complete list, follow this [link](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html). Uncomment the below code to have the complete upenn tagset used in the nltk package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "de25b391",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('tagsets')\n",
    "#print(nltk.help.upenn_tagset())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b18fb0",
   "metadata": {},
   "source": [
    "### Example 1 - Reading the Newspaper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9e88153e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Democrats took exceedingly narrow control of the Senate on Wednesday after winning both runoff elections in Georgia, granting them control of Congress and the White House for the first time since 2011. Democrat Jon Ossoff defeated Republican David Perdue, according to The Associated Press, making him the youngest member of the U.S. Senate and the first Jewish senator from Georgia. Earlier Raphael Warnock, a pastor from Atlanta, defeated GOP Sen. Kelly Loeffler after a bitter campaign. Warnock becomes the first Black Democrat elected to the Senate from a Southern state.The Senate will now be split 50-50 between the two parties, giving Vice President-elect Kamala Harris the tiebreaking vote.\n"
     ]
    }
   ],
   "source": [
    "article = \"Democrats took exceedingly narrow control of the Senate on Wednesday after winning both runoff elections in Georgia, granting them control of Congress and the White House for the first time since 2011. Democrat Jon Ossoff defeated Republican David Perdue, according to The Associated Press, making him the youngest member of the U.S. Senate and the first Jewish senator from Georgia. Earlier Raphael Warnock, a pastor from Atlanta, defeated GOP Sen. Kelly Loeffler after a bitter campaign. Warnock becomes the first Black Democrat elected to the Senate from a Southern state.The Senate will now be split 50-50 between the two parties, giving Vice President-elect Kamala Harris the tiebreaking vote.\"\n",
    "print(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb39979",
   "metadata": {},
   "source": [
    "At a first glance as \"human beings\", we can see that in the extract there are different names of people and also of organizations; although it is possible that the names are completely new to us, we can recognize without a shadow of a doubt that \"Kamala Harris\" is the name of a person of female gender, just as \"Associated Press\" could be the name of an organization or a conference. Now, we want the same result to be produced by our system. Like? Through a function that takes the sentence as input and is able to tokenize the content and extract the grammatical types. \n",
    "\n",
    "We can use the function ***pos_tag*** of the ***NLTK package***; it is in fact able to take an array of words and assign them a label that determines the type of content that each of them represents at the grammatical level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "501f9de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Democrats', 'NNPS'), ('took', 'VBD'), ('exceedingly', 'RB'), ('narrow', 'JJ'), ('control', 'NN'), ('of', 'IN'), ('the', 'DT'), ('Senate', 'NNP'), ('on', 'IN'), ('Wednesday', 'NNP'), ('after', 'IN'), ('winning', 'VBG'), ('both', 'DT'), ('runoff', 'NN'), ('elections', 'NNS'), ('in', 'IN'), ('Georgia', 'NNP'), (',', ','), ('granting', 'VBG'), ('them', 'PRP'), ('control', 'NN'), ('of', 'IN'), ('Congress', 'NNP'), ('and', 'CC'), ('the', 'DT'), ('White', 'NNP'), ('House', 'NNP'), ('for', 'IN'), ('the', 'DT'), ('first', 'JJ'), ('time', 'NN'), ('since', 'IN'), ('2011', 'CD'), ('.', '.'), ('Democrat', 'NNP'), ('Jon', 'NNP'), ('Ossoff', 'NNP'), ('defeated', 'VBD'), ('Republican', 'NNP'), ('David', 'NNP'), ('Perdue', 'NNP'), (',', ','), ('according', 'VBG'), ('to', 'TO'), ('The', 'DT'), ('Associated', 'NNP'), ('Press', 'NNP'), (',', ','), ('making', 'VBG'), ('him', 'PRP'), ('the', 'DT'), ('youngest', 'JJS'), ('member', 'NN'), ('of', 'IN'), ('the', 'DT'), ('U.S.', 'NNP'), ('Senate', 'NNP'), ('and', 'CC'), ('the', 'DT'), ('first', 'JJ'), ('Jewish', 'JJ'), ('senator', 'NN'), ('from', 'IN'), ('Georgia', 'NNP'), ('.', '.'), ('Earlier', 'RBR'), ('Raphael', 'NNP'), ('Warnock', 'NNP'), (',', ','), ('a', 'DT'), ('pastor', 'NN'), ('from', 'IN'), ('Atlanta', 'NNP'), (',', ','), ('defeated', 'VBD'), ('GOP', 'NNP'), ('Sen.', 'NNP'), ('Kelly', 'NNP'), ('Loeffler', 'NNP'), ('after', 'IN'), ('a', 'DT'), ('bitter', 'JJ'), ('campaign', 'NN'), ('.', '.'), ('Warnock', 'NNP'), ('becomes', 'VBZ'), ('the', 'DT'), ('first', 'JJ'), ('Black', 'NNP'), ('Democrat', 'NNP'), ('elected', 'VBD'), ('to', 'TO'), ('the', 'DT'), ('Senate', 'NNP'), ('from', 'IN'), ('a', 'DT'), ('Southern', 'NNP'), ('state.The', 'NN'), ('Senate', 'NNP'), ('will', 'MD'), ('now', 'RB'), ('be', 'VB'), ('split', 'VBN'), ('50-50', 'JJ'), ('between', 'IN'), ('the', 'DT'), ('two', 'CD'), ('parties', 'NNS'), (',', ','), ('giving', 'VBG'), ('Vice', 'NNP'), ('President-elect', 'NNP'), ('Kamala', 'NNP'), ('Harris', 'NNP'), ('the', 'DT'), ('tiebreaking', 'JJ'), ('vote', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "tagged_tokens = nltk.word_tokenize(article)\n",
    "tagged_tokens = nltk.pos_tag(tagged_tokens)\n",
    "print(tagged_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b52e729",
   "metadata": {},
   "source": [
    "As we have already seen, at a first glance, everything appears very confusing, but in reality each of those acronyms has a very specific meaning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4f7dde",
   "metadata": {},
   "source": [
    "<p><strong>NLTK POS Tags Examples are as Below:</strong></p>\n",
    "<table class=\"table table-striped\">\n",
    "<thead>\n",
    "<tr>\n",
    "<th  style=\"text-align: left;\">Abbreviation</th>\n",
    "<th  style=\"text-align: left;\">Meaning</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tr>\n",
    "<td style=\"text-align: left;\">CC\n",
    "</td>\n",
    "<td style=\"text-align: left;\">coordinating conjunction\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"text-align: left;\">CD\n",
    "</td>\n",
    "<td style=\"text-align: left;\">cardinal digit\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"text-align: left;\">DT\n",
    "</td>\n",
    "<td style=\"text-align: left;\">determiner\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"text-align: left;\">EX\n",
    "</td>\n",
    "<td  style=\"text-align: left;\">existential there\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td  style=\"text-align: left;\">FW\n",
    "</td>\n",
    "<td  style=\"text-align: left;\">foreign word\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td  style=\"text-align: left;\">IN\n",
    "</td>\n",
    "<td  style=\"text-align: left;\">preposition/subordinating conjunction\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td  style=\"text-align: left;\">JJ\n",
    "</td>\n",
    "<td  style=\"text-align: left;\">This NLTK POS Tag is an adjective (large)\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td  style=\"text-align: left;\">JJR\n",
    "</td>\n",
    "<td  style=\"text-align: left;\">adjective, comparative (larger)\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td  style=\"text-align: left;\">JJS\n",
    "</td>\n",
    "<td  style=\"text-align: left;\">adjective, superlative (largest)\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td  style=\"text-align: left;\">LS\n",
    "</td>\n",
    "<td  style=\"text-align: left;\">list market\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td  style=\"text-align: left;\">MD\n",
    "</td>\n",
    "<td  style=\"text-align: left;\">modal (could, will)\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td  style=\"text-align: left;\">NN\n",
    "</td>\n",
    "<td  style=\"text-align: left;\">noun, singular (cat, tree)\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td  style=\"text-align: left;\">NNS\n",
    "</td>\n",
    "<td  style=\"text-align: left;\">noun plural (desks)\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td  style=\"text-align: left;\">NNP\n",
    "</td>\n",
    "<td  style=\"text-align: left;\">proper noun, singular (sarah)\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td  style=\"text-align: left;\">NNPS\n",
    "</td>\n",
    "<td  style=\"text-align: left;\">proper noun, plural (indians or americans)\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td  style=\"text-align: left;\">PDT\n",
    "</td>\n",
    "<td  style=\"text-align: left;\">predeterminer (all, both, half)\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td  style=\"text-align: left;\">POS\n",
    "</td>\n",
    "<td  style=\"text-align: left;\">possessive ending  (parent\\ &#8216;s)\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td  style=\"text-align: left;\">PRP\n",
    "</td>\n",
    "<td  style=\"text-align: left;\">personal pronoun (hers, herself, him,himself)\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td  style=\"text-align: left;\">PRP$\n",
    "</td>\n",
    "<td  style=\"text-align: left;\">possessive pronoun (her, his, mine, my, our )\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td  style=\"text-align: left;\">RB\n",
    "</td>\n",
    "<td  style=\"text-align: left;\">adverb (occasionally, swiftly)\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td  style=\"text-align: left;\">RBR\n",
    "</td>\n",
    "<td  style=\"text-align: left;\">adverb, comparative (greater)\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td  style=\"text-align: left;\">RBS\n",
    "</td>\n",
    "<td  style=\"text-align: left;\">adverb, superlative (biggest)\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td  style=\"text-align: left;\">RP\n",
    "</td>\n",
    "<td  style=\"text-align: left;\">particle (about)\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td  style=\"text-align: left;\">TO\n",
    "</td>\n",
    "<td  style=\"text-align: left;\">infinite marker (to)\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td  style=\"text-align: left;\">UH\n",
    "</td>\n",
    "<td  style=\"text-align: left;\">interjection (goodbye)\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td  style=\"text-align: left;\">VB\n",
    "</td>\n",
    "<td  style=\"text-align: left;\">verb (ask)\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td  style=\"text-align: left;\">VBG\n",
    "</td>\n",
    "<td  style=\"text-align: left;\">verb gerund (judging)\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td  style=\"text-align: left;\">VBD\n",
    "</td>\n",
    "<td  style=\"text-align: left;\">verb past tense (pleaded)\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td  style=\"text-align: left;\">VBN\n",
    "</td>\n",
    "<td  style=\"text-align: left;\">verb past participle (reunified)\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td  style=\"text-align: left;\">VBP\n",
    "</td>\n",
    "<td  style=\"text-align: left;\">verb, present tense not 3rd person singular(wrap)\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td  style=\"text-align: left;\">VBZ\n",
    "</td>\n",
    "<td  style=\"text-align: left;\">verb, present tense with 3rd person singular (bases)\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td  style=\"text-align: left;\">WDT\n",
    "</td>\n",
    "<td  style=\"text-align: left;\">wh-determiner (that, what)\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td  style=\"text-align: left;\">WP\n",
    "</td>\n",
    "<td  style=\"text-align: left;\">wh- pronoun (who)\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td  style=\"text-align: left;\">WRB\n",
    "</td>\n",
    "<td  style=\"text-align: left;\">wh- adverb (how)\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb99a19b",
   "metadata": {},
   "source": [
    "### Where we can use POST\n",
    "\n",
    "Part-of-speech tags describe the characteristic structure of lexical terms within a sentence or text, therefore, we can use them for making assumptions about semantics. Other applications of POS tagging include:\n",
    "\n",
    "- **Named Entity Recognition and Chunking (see below)**\n",
    "- Co-reference Resolution\n",
    "- Speech Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe8ebfa",
   "metadata": {},
   "source": [
    "### Build your own POS Tagger "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf596d5",
   "metadata": {},
   "source": [
    "It is interesting to understand how to create your own POS tagger using a supervised learning method. This will also allow us to understand the role played by features in identifying individual parts of the text. Finally we will see how the context plays a fundamental role for a correct classification. For this exercise we focus out attenction on suffixes. We'll train a classifier to work out which suffixes are most informative. The training sample will be build from the Brown Corpus which is part of the NLTK package.\n",
    "\n",
    "The Brown Corpus was the first million-word electronic corpus of English, created in 1961 at Brown University. This corpus contains text from 500 sources, and the sources have been categorized by genre, such as news, editorial, and so on.  (for a complete list, see http://icame.uib.no/brown/bcm-los.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "01504d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b81add",
   "metadata": {},
   "source": [
    "Let's begin by finding out what the most common suffixes are. For this we are going to use a FreqDist() object from the NLTK package.\n",
    "\n",
    "A frequency distribution counter records the number of times each outcome of an experiment has occurred. For example, a frequency distribution could be used to record the frequency of each word type in a document. Formally, a frequency distribution can be defined as a function mapping from each sample to the number of times that sample occurred as an outcome.\n",
    "\n",
    "Frequency distributions are generally constructed by running a number of experiments, and incrementing the count for a sample every time it is an outcome of an experiment. For example, the following code will produce a frequency distribution of any n-gram with $1 \\le n \\le 3$ appearing at the end of a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "55d04bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix_fdist = nltk.FreqDist()\n",
    "\n",
    "for word in brown.words():\n",
    "    word = word.lower()\n",
    "    suffix_fdist[word[-1:]] += 1   # single character at the end of the word\n",
    "    suffix_fdist[word[-2:]] += 1   # bi-gram\n",
    "    suffix_fdist[word[-3:]] += 1   # three-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "96ce4169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'e': 202946, ',': 175002, '.': 152999, 's': 128722, 'd': 105687, 't': 94459, 'he': 92084, 'n': 87889, 'a': 74912, 'of': 72978, ...})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suffix_fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "16962cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['e', ',', '.', 's', 'd', 't', 'he', 'n', 'a', 'of', 'the', 'y', 'r', 'to', 'in', 'f', 'o', 'ed', 'nd', 'is', 'on', 'l', 'g', 'and', 'ng', 'er', 'as', 'ing', 'h', 'at', 'es', 'or', 're', 'it', '``', 'an', \"''\", 'm', ';', 'i', 'ly', 'ion', 'en', 'al', '?', 'nt', 'be', 'hat', 'st', 'his', 'th', 'll', 'le', 'ce', 'by', 'ts', 'me', 've', \"'\", 'se', 'ut', 'was', 'for', 'ent', 'ch', 'k', 'w', 'ld', '`', 'rs', 'ted', 'ere', 'her', 'ne', 'ns', 'ith', 'ad', 'ry', ')', '(', 'te', '--', 'ay', 'ty', 'ot', 'p', 'nce', \"'s\", 'ter', 'om', 'ss', ':', 'we', 'are', 'c', 'ers', 'uld', 'had', 'so', 'ey']\n"
     ]
    }
   ],
   "source": [
    "common_suffixes = [suffix for (suffix, count) in suffix_fdist.most_common(100)]\n",
    "print(common_suffixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808d67e7",
   "metadata": {},
   "source": [
    "Next, we'll define a feature extractor function which checks a given word for these suffixes. For this we are going to use the `endswith()` method of `String` class, that returns `True` if the string ends with the specified value, otherwise `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ef61c19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = \"processing\"\n",
    "word.endswith(\"ing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "feed9329",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_features(word):\n",
    "    features = {}\n",
    "    for suffix in common_suffixes:\n",
    "        features['endswith({})'.format(suffix)] = word.lower().endswith(suffix)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0007ba31",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_words = brown.tagged_words(categories='news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb9b12aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'AT'),\n",
       " ('Fulton', 'NP-TL'),\n",
       " ('County', 'NN-TL'),\n",
       " ('Grand', 'JJ-TL'),\n",
       " ('Jury', 'NN-TL'),\n",
       " ('said', 'VBD'),\n",
       " ('Friday', 'NR'),\n",
       " ('an', 'AT'),\n",
       " ('investigation', 'NN'),\n",
       " ('of', 'IN')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca21681f",
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(pos_features(n), g) for (n,g) in tagged_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "14fc3762",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = int(len(featuresets) * 0.1)\n",
    "train_set, test_set = featuresets[size:], featuresets[:size]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31097a5f",
   "metadata": {},
   "source": [
    "Now that we've defined our feature extractor, we can use it to train a new **decision tree** classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9437a3f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6270512182993535"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = nltk.DecisionTreeClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbc4a2e",
   "metadata": {},
   "source": [
    "#### Exploiting Context\n",
    "\n",
    "By augmenting the feature extraction function, we could modify this part-of-speech tagger to leverage a variety of other word-internal features, such as the length of the word, the number of syllables it contains, or its prefix. However, as long as the feature extractor just looks at the target word, we have no way to add features that depend on the context that the word appears in. But contextual features often provide powerful clues about the correct tag — for example, when tagging the word \"fly,\" knowing that the previous word is \"a\" will allow us to determine that it is functioning as a noun, not a verb.\n",
    "\n",
    "In order to accommodate features that depend on a word's context, we must revise the pattern that we used to define our feature extractor. Instead of just passing in the word to be tagged, we will pass in a complete (untagged) sentence, along with the index of the target word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1bfe97b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_features_2(sentence, i):\n",
    "    features = {\"suffix(1)\": sentence[i][-1:],\n",
    "                \"suffix(2)\": sentence[i][-2:],\n",
    "                \"suffix(3)\": sentence[i][-3:]}\n",
    "    if i == 0:\n",
    "        features[\"prev-word\"] = \"<START>\"\n",
    "    else:\n",
    "        features[\"prev-word\"] = sentence[i-1]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "97d4d252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ('Grand', 'JJ-TL'), ('Jury', 'NN-TL'), ('said', 'VBD'), ('Friday', 'NR'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'IN'), (\"Atlanta's\", 'NP$'), ('recent', 'JJ'), ('primary', 'NN'), ('election', 'NN'), ('produced', 'VBD'), ('``', '``'), ('no', 'AT'), ('evidence', 'NN'), (\"''\", \"''\"), ('that', 'CS'), ('any', 'DTI'), ('irregularities', 'NNS'), ('took', 'VBD'), ('place', 'NN'), ('.', '.')]\n",
      "('Fulton', 'NP-TL')\n"
     ]
    }
   ],
   "source": [
    "tagged_sents = brown.tagged_sents(categories='news')\n",
    "print(tagged_sents[0])\n",
    "print(tagged_sents[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "00f6dbb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ('The', 'AT')\n",
      "1 ('Fulton', 'NP-TL')\n",
      "2 ('County', 'NN-TL')\n",
      "3 ('Grand', 'JJ-TL')\n",
      "4 ('Jury', 'NN-TL')\n",
      "5 ('said', 'VBD')\n",
      "6 ('Friday', 'NR')\n",
      "7 ('an', 'AT')\n",
      "8 ('investigation', 'NN')\n",
      "9 ('of', 'IN')\n",
      "10 (\"Atlanta's\", 'NP$')\n",
      "11 ('recent', 'JJ')\n",
      "12 ('primary', 'NN')\n",
      "13 ('election', 'NN')\n",
      "14 ('produced', 'VBD')\n",
      "15 ('``', '``')\n",
      "16 ('no', 'AT')\n",
      "17 ('evidence', 'NN')\n",
      "18 (\"''\", \"''\")\n",
      "19 ('that', 'CS')\n",
      "20 ('any', 'DTI')\n",
      "21 ('irregularities', 'NNS')\n",
      "22 ('took', 'VBD')\n",
      "23 ('place', 'NN')\n",
      "24 ('.', '.')\n"
     ]
    }
   ],
   "source": [
    "for (word, tag) in enumerate(tagged_sents[0]):\n",
    "    print(word, tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0070d230",
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = []\n",
    "for tagged_sent in tagged_sents:\n",
    "    untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "    for i, (word, tag) in enumerate(tagged_sent):\n",
    "        featuresets.append( (pos_features_2(untagged_sent, i), tag) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0ff638a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'suffix(1)': 'e', 'suffix(2)': 'he', 'suffix(3)': 'The', 'prev-word': '<START>'}, 'AT')\n"
     ]
    }
   ],
   "source": [
    "print(featuresets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b05b6b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = int(len(featuresets) * 0.1)\n",
    "train_set, test_set = featuresets[size:], featuresets[:size]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "97cfdae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7891596220785678"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb04a5e",
   "metadata": {},
   "source": [
    "It is clear that exploiting contextual features improves the performance of our part-of-speech tagger. For example, the classifier learns that a word is likely to be a noun if it comes immediately after the word \"large\". However, it is unable to learn the generalization that a word is probably a noun if it follows an adjective, because it doesn't have access to the previous word's part-of-speech tag. In general, simple classifiers always treat each input as independent from all other inputs. In many contexts, this makes perfect sense. For example, decisions about whether names tend to be male or female can be made on a case-by-case basis. However, there are often cases, such as part-of-speech tagging, where we are interested in solving classification problems that are closely related to one another. [rinviare al paragrafo di nltk per i sequence classificators]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eadbbe6",
   "metadata": {},
   "source": [
    "## spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ac3e46",
   "metadata": {},
   "source": [
    "### What is spaCy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ea0240",
   "metadata": {},
   "source": [
    "[spaCy](https://spacy.io/) is another free, open-source library for advanced Natural Language Processing (NLP) in Python.\n",
    "\n",
    "spaCy is designed specifically for production use and helps you build applications that process and “understand” large volumes of text. It can be used to build information extraction or natural language understanding systems. It is a fully optimized and  is widely used in deep learning.\n",
    "\n",
    "Both libraries (spaCy and NLTK) are excellent and very well managed and documented. Any of them could be a perfect choice, depending on the characteristics of your natural language processing projects. NLTK is a string processing library: it takes strings as input and returns the resulting string as output; spaCy uses an object-oriented approach. As a result spaCy returns an object in the form of a document, with words or phrases. NLTK provides multiple algorithms for solving a problem, while spaCy only provides the best algorithm for solving a given problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139233cf",
   "metadata": {},
   "source": [
    "We will not go into the details of this library in this introductory course. However we want to point out that the efficiency of the underlying deep learning models often make it the best choice for industrial applications. \n",
    "\n",
    "We will use some of the trained pipelines in the last part of this seminar where we talk about extracting information from unstructured data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "05b6fec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text     POS    TAG    Dep    POS explained        tag explained \n",
      "Democrats PROPN  NNPS   nsubj  proper noun          noun, proper plural\n",
      "took     VERB   VBD    ROOT   verb                 verb, past tense\n",
      "exceedingly ADV    RB     advmod adverb               adverb\n",
      "narrow   ADJ    JJ     amod   adjective            adjective\n",
      "control  NOUN   NN     dobj   noun                 noun, singular or mass\n",
      "of       ADP    IN     prep   adposition           conjunction, subordinating or preposition\n",
      "the      DET    DT     det    determiner           determiner\n",
      "Senate   PROPN  NNP    pobj   proper noun          noun, proper singular\n",
      "on       ADP    IN     prep   adposition           conjunction, subordinating or preposition\n",
      "Wednesday PROPN  NNP    pobj   proper noun          noun, proper singular\n",
      "after    ADP    IN     prep   adposition           conjunction, subordinating or preposition\n",
      "winning  VERB   VBG    pcomp  verb                 verb, gerund or present participle\n",
      "both     DET    DT     det    determiner           determiner\n",
      "runoff   NOUN   NN     compound noun                 noun, singular or mass\n",
      "elections NOUN   NNS    dobj   noun                 noun, plural\n",
      "in       ADP    IN     prep   adposition           conjunction, subordinating or preposition\n",
      "Georgia  PROPN  NNP    pobj   proper noun          noun, proper singular\n",
      ",        PUNCT  ,      punct  punctuation          punctuation mark, comma\n",
      "granting VERB   VBG    conj   verb                 verb, gerund or present participle\n",
      "them     PRON   PRP    dative pronoun              pronoun, personal\n",
      "control  NOUN   NN     dobj   noun                 noun, singular or mass\n",
      "of       ADP    IN     prep   adposition           conjunction, subordinating or preposition\n",
      "Congress PROPN  NNP    pobj   proper noun          noun, proper singular\n",
      "and      CCONJ  CC     cc     coordinating conjunction conjunction, coordinating\n",
      "the      DET    DT     det    determiner           determiner\n",
      "White    PROPN  NNP    compound proper noun          noun, proper singular\n",
      "House    PROPN  NNP    conj   proper noun          noun, proper singular\n",
      "for      ADP    IN     prep   adposition           conjunction, subordinating or preposition\n",
      "the      DET    DT     det    determiner           determiner\n",
      "first    ADJ    JJ     amod   adjective            adjective\n",
      "time     NOUN   NN     pobj   noun                 noun, singular or mass\n",
      "since    SCONJ  IN     prep   subordinating conjunction conjunction, subordinating or preposition\n",
      "2011     NUM    CD     pobj   numeral              cardinal number\n",
      ".        PUNCT  .      punct  punctuation          punctuation mark, sentence closer\n",
      "Democrat PROPN  NNP    compound proper noun          noun, proper singular\n",
      "Jon      PROPN  NNP    compound proper noun          noun, proper singular\n",
      "Ossoff   PROPN  NNP    nsubj  proper noun          noun, proper singular\n",
      "defeated VERB   VBD    ROOT   verb                 verb, past tense\n",
      "Republican PROPN  NNP    compound proper noun          noun, proper singular\n",
      "David    PROPN  NNP    compound proper noun          noun, proper singular\n",
      "Perdue   PROPN  NNP    dobj   proper noun          noun, proper singular\n",
      ",        PUNCT  ,      punct  punctuation          punctuation mark, comma\n",
      "according VERB   VBG    prep   verb                 verb, gerund or present participle\n",
      "to       ADP    IN     prep   adposition           conjunction, subordinating or preposition\n",
      "The      DET    DT     det    determiner           determiner\n",
      "Associated PROPN  NNP    compound proper noun          noun, proper singular\n",
      "Press    PROPN  NNP    pobj   proper noun          noun, proper singular\n",
      ",        PUNCT  ,      punct  punctuation          punctuation mark, comma\n",
      "making   VERB   VBG    advcl  verb                 verb, gerund or present participle\n",
      "him      PRON   PRP    nsubj  pronoun              pronoun, personal\n",
      "the      DET    DT     det    determiner           determiner\n",
      "youngest ADJ    JJS    amod   adjective            adjective, superlative\n",
      "member   NOUN   NN     ccomp  noun                 noun, singular or mass\n",
      "of       ADP    IN     prep   adposition           conjunction, subordinating or preposition\n",
      "the      DET    DT     det    determiner           determiner\n",
      "U.S.     PROPN  NNP    compound proper noun          noun, proper singular\n",
      "Senate   PROPN  NNP    pobj   proper noun          noun, proper singular\n",
      "and      CCONJ  CC     cc     coordinating conjunction conjunction, coordinating\n",
      "the      DET    DT     det    determiner           determiner\n",
      "first    ADJ    JJ     amod   adjective            adjective\n",
      "Jewish   ADJ    JJ     amod   adjective            adjective\n",
      "senator  NOUN   NN     conj   noun                 noun, singular or mass\n",
      "from     ADP    IN     prep   adposition           conjunction, subordinating or preposition\n",
      "Georgia  PROPN  NNP    pobj   proper noun          noun, proper singular\n",
      ".        PUNCT  .      punct  punctuation          punctuation mark, sentence closer\n",
      "Earlier  ADV    RBR    amod   adverb               adverb, comparative\n",
      "Raphael  PROPN  NNP    compound proper noun          noun, proper singular\n",
      "Warnock  PROPN  NNP    nsubj  proper noun          noun, proper singular\n",
      ",        PUNCT  ,      punct  punctuation          punctuation mark, comma\n",
      "a        DET    DT     det    determiner           determiner\n",
      "pastor   NOUN   NN     appos  noun                 noun, singular or mass\n",
      "from     ADP    IN     prep   adposition           conjunction, subordinating or preposition\n",
      "Atlanta  PROPN  NNP    pobj   proper noun          noun, proper singular\n",
      ",        PUNCT  ,      punct  punctuation          punctuation mark, comma\n",
      "defeated VERB   VBD    ROOT   verb                 verb, past tense\n",
      "GOP      PROPN  NNP    compound proper noun          noun, proper singular\n",
      "Sen.     PROPN  NNP    compound proper noun          noun, proper singular\n",
      "Kelly    PROPN  NNP    compound proper noun          noun, proper singular\n",
      "Loeffler PROPN  NNP    dobj   proper noun          noun, proper singular\n",
      "after    ADP    IN     prep   adposition           conjunction, subordinating or preposition\n",
      "a        DET    DT     det    determiner           determiner\n",
      "bitter   ADJ    JJ     amod   adjective            adjective\n",
      "campaign NOUN   NN     pobj   noun                 noun, singular or mass\n",
      ".        PUNCT  .      punct  punctuation          punctuation mark, sentence closer\n",
      "Warnock  NOUN   NN     nsubj  noun                 noun, singular or mass\n",
      "becomes  VERB   VBZ    ROOT   verb                 verb, 3rd person singular present\n",
      "the      DET    DT     det    determiner           determiner\n",
      "first    ADJ    JJ     amod   adjective            adjective\n",
      "Black    PROPN  NNP    compound proper noun          noun, proper singular\n",
      "Democrat PROPN  NNP    attr   proper noun          noun, proper singular\n",
      "elected  VERB   VBD    acl    verb                 verb, past tense\n",
      "to       ADP    IN     prep   adposition           conjunction, subordinating or preposition\n",
      "the      DET    DT     det    determiner           determiner\n",
      "Senate   PROPN  NNP    pobj   proper noun          noun, proper singular\n",
      "from     ADP    IN     prep   adposition           conjunction, subordinating or preposition\n",
      "a        DET    DT     det    determiner           determiner\n",
      "Southern ADJ    JJ     amod   adjective            adjective\n",
      "state    NOUN   NN     pobj   noun                 noun, singular or mass\n",
      ".        PUNCT  .      punct  punctuation          punctuation mark, sentence closer\n",
      "The      DET    DT     det    determiner           determiner\n",
      "Senate   PROPN  NNP    nsubjpass proper noun          noun, proper singular\n",
      "will     VERB   MD     aux    verb                 verb, modal auxiliary\n",
      "now      ADV    RB     advmod adverb               adverb\n",
      "be       AUX    VB     auxpass auxiliary            verb, base form\n",
      "split    VERB   VBN    ROOT   verb                 verb, past participle\n",
      "50       NUM    CD     npadvmod numeral              cardinal number\n",
      "-        SYM    SYM    punct  symbol               symbol\n",
      "50       NUM    CD     prep   numeral              cardinal number\n",
      "between  ADP    IN     prep   adposition           conjunction, subordinating or preposition\n",
      "the      DET    DT     det    determiner           determiner\n",
      "two      NUM    CD     nummod numeral              cardinal number\n",
      "parties  NOUN   NNS    pobj   noun                 noun, plural\n",
      ",        PUNCT  ,      punct  punctuation          punctuation mark, comma\n",
      "giving   VERB   VBG    advcl  verb                 verb, gerund or present participle\n",
      "Vice     PROPN  NNP    compound proper noun          noun, proper singular\n",
      "President PROPN  NNP    compound proper noun          noun, proper singular\n",
      "-        PUNCT  HYPH   punct  punctuation          punctuation mark, hyphen\n",
      "elect    PROPN  NNP    compound proper noun          noun, proper singular\n",
      "Kamala   PROPN  NNP    compound proper noun          noun, proper singular\n",
      "Harris   PROPN  NNP    dative proper noun          noun, proper singular\n",
      "the      DET    DT     det    determiner           determiner\n",
      "tiebreaking VERB   VBG    amod   verb                 verb, gerund or present participle\n",
      "vote     NOUN   NN     dobj   noun                 noun, singular or mass\n",
      ".        PUNCT  .      punct  punctuation          punctuation mark, sentence closer\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(article)\n",
    "\n",
    "print(f\"{'text':{8}} {'POS':{6}} {'TAG':{6}} {'Dep':{6}} {'POS explained':{20}} {'tag explained'} \")\n",
    "for token in doc:\n",
    "    print(f'{token.text:{8}} {token.pos_:{6}} {token.tag_:{6}} {token.dep_:{6}} {spacy.explain(token.pos_):{20}} {spacy.explain(token.tag_)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b2f216",
   "metadata": {},
   "source": [
    "## Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5bd815",
   "metadata": {},
   "source": [
    "Named Entity Recognition (NER) is a natural language processing technique that uses machine learning to identify named entities in text data and classifies them into one or more predetermined categories. Entities can be names of people, organizations, locations, topics, interests, and more. Named Entity Recognition is used to automatically categorize news articles, customer feedback, customer support tickets, social media posts, and resumes.\n",
    "\n",
    "Natural language-based named entity recognition enables you to effectively search unstructured data, to improve process efficiency, and create better personalized customer experiences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fade934",
   "metadata": {},
   "source": [
    "The goal of a named entity recognition (NER) system is to identify all textual mentions of the named entities. This can be broken down into two sub-tasks: identifying the boundaries of the NE, and identifying its type. \n",
    "\n",
    "How do we go about identifying named entities? One option would be to look up each word in an appropriate list of names. For example, in the case of locations, we could use a gazetteer, or geographical dictionary. However, doing this blindly runs into problems because many named entity terms are ambiguous. Thus **May** and **North** are likely to be parts of named entities for **DATE** and **LOCATION**, respectively, but could both be part of a PERSON; conversely **Christian Dior** looks like a **PERSON** but is more likely to be of type **ORGANIZATION**. A term like Yankee will be ordinary modifier in some contexts, but will be marked as an entity of type ORGANIZATION in the phrase Yankee infielders.\n",
    "\n",
    "Further challenges are posed by multi-word names like **Stanford University**, and by names that contain other names such as Cecil H. Green Library and Escondido Village Conference Service Center. In named entity recognition, therefore, we need to be able to identify the beginning and end of multi-token sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16f57ee",
   "metadata": {},
   "source": [
    "## Example 2 - Reading Again the Newspaper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d552223",
   "metadata": {},
   "source": [
    "(See ***Example 1*** above)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936c4754",
   "metadata": {},
   "source": [
    "Goal : Finding People and Organizations Cited in a Newspaper Article. \n",
    "\n",
    "Python libraries used: \n",
    "- nltk; \n",
    "- spaCy; \n",
    "- Stanford NER."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f698f6d",
   "metadata": {},
   "source": [
    "#### Using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60f335d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Democrats took exceedingly narrow control of the Senate on Wednesday after winning both runoff elections in Georgia, granting them control of Congress and the White House for the first time since 2011. Democrat Jon Ossoff defeated Republican David Perdue, according to The Associated Press, making him the youngest member of the U.S. Senate and the first Jewish senator from Georgia. Earlier Raphael Warnock, a pastor from Atlanta, defeated GOP Sen. Kelly Loeffler after a bitter campaign. Warnock becomes the first Black Democrat elected to the Senate from a Southern state.The Senate will now be split 50-50 between the two parties, giving Vice President-elect Kamala Harris the tiebreaking vote.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "article = \"Democrats took exceedingly narrow control of the Senate on Wednesday after winning both runoff elections in Georgia, granting them control of Congress and the White House for the first time since 2011. Democrat Jon Ossoff defeated Republican David Perdue, according to The Associated Press, making him the youngest member of the U.S. Senate and the first Jewish senator from Georgia. Earlier Raphael Warnock, a pastor from Atlanta, defeated GOP Sen. Kelly Loeffler after a bitter campaign. Warnock becomes the first Black Democrat elected to the Senate from a Southern state.The Senate will now be split 50-50 between the two parties, giving Vice President-elect Kamala Harris the tiebreaking vote.\"\n",
    "print(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f95450e",
   "metadata": {},
   "source": [
    "NLTK works very well with Stanford’s efficient NER implementation, which is a Java implementation of a NER system. This comes with well-designed function extractors for recognizing named entities and many options for defining function extractors, especially for the three classes (“person”, “organization”, “location”). \n",
    "\n",
    "We then import the tools to use the system produced by [Stanford NLP Group](https://nlp.stanford.edu/software/)  and see the result produced in output. Stanford NER is a Java implementation of a Named Entity Recognizer. Named Entity Recognition (NER) labels sequences of words in a text which are the names of things, such as person and company names, or gene and protein names. It comes with well-engineered feature extractors for Named Entity Recognition, and many options for defining feature extractors. Included with the download are good named entity recognizers for English, particularly for the 3 classes (PERSON, ORGANIZATION, LOCATION). The theory behind how this library works is quite complicated and we won't go into it further in these introductory notes. For those of you interested in the theory at this [link](https://nlp.stanford.edu/software/CRF-NER.html) you will find all the references as well as the instructions to download and install the java software."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f43ee952",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import StanfordNERTagger\n",
    "# In order to make the following code work you need to install java sdk 8.0+ on your machine and make sure that \n",
    "# the path of the java executable is saved in the JAVAHOME environment variable\n",
    "stanford_ner_tagger = StanfordNERTagger('./stanford-ner-2020-11-17/' + 'classifiers/english.muc.7class.distsim.crf.ser.gz',\n",
    "                                        './stanford-ner-2020-11-17/' + 'stanford-ner-4.2.0.jar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19f26f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = stanford_ner_tagger.tag(article.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "918be8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original article: Democrats took exceedingly narrow control of the Senate on Wednesday after winning both runoff elections in Georgia, granting them control of Congress and the White House for the first time since 2011. Democrat Jon Ossoff defeated Republican David Perdue, according to The Associated Press, making him the youngest member of the U.S. Senate and the first Jewish senator from Georgia. Earlier Raphael Warnock, a pastor from Atlanta, defeated GOP Sen. Kelly Loeffler after a bitter campaign. Warnock becomes the first Black Democrat elected to the Senate from a Southern state.The Senate will now be split 50-50 between the two parties, giving Vice President-elect Kamala Harris the tiebreaking vote. \n",
      "\n",
      "ER Type : ORGANIZATION, Value : Senate\n",
      "ER Type : DATE, Value : Wednesday\n",
      "ER Type : ORGANIZATION, Value : Congress\n",
      "ER Type : ORGANIZATION, Value : White\n",
      "ER Type : ORGANIZATION, Value : House\n",
      "ER Type : PERSON, Value : Jon\n",
      "ER Type : PERSON, Value : Ossoff\n",
      "ER Type : PERSON, Value : David\n",
      "ER Type : PERSON, Value : Perdue,\n",
      "ER Type : ORGANIZATION, Value : U.S.\n",
      "ER Type : ORGANIZATION, Value : Senate\n",
      "ER Type : ORGANIZATION, Value : Georgia.\n",
      "ER Type : ORGANIZATION, Value : Earlier\n",
      "ER Type : ORGANIZATION, Value : Raphael\n",
      "ER Type : ORGANIZATION, Value : Warnock,\n",
      "ER Type : PERSON, Value : Kelly\n",
      "ER Type : PERSON, Value : Loeffler\n",
      "ER Type : PERSON, Value : Warnock\n",
      "ER Type : ORGANIZATION, Value : Senate\n",
      "ER Type : ORGANIZATION, Value : Southern\n",
      "ER Type : ORGANIZATION, Value : state.The\n",
      "ER Type : ORGANIZATION, Value : Senate\n",
      "ER Type : PERSON, Value : Kamala\n",
      "ER Type : PERSON, Value : Harris\n"
     ]
    }
   ],
   "source": [
    "print('Original article: %s \\n' % (article))\n",
    "for result in results:\n",
    "    value = result[0]\n",
    "    tag   = result[1]\n",
    "    if tag != 'O':\n",
    "        print('ER Type : %s, Value : %s' % (tag, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1575609",
   "metadata": {},
   "source": [
    "#### Using spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24dfa034",
   "metadata": {},
   "source": [
    "We can also try to perform the same process with the library, spaCy, which allows the support of a greater number of types of entities, such as person, nationality, buildings, events and so on. Let's modify the code as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20f61a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfe135a",
   "metadata": {},
   "source": [
    "First of all we have to download and install a trained pipeline (in this case `en_core_web_sm`), you can load it via spacy.load. This will return a Language object containing all components and data needed to process text. We call it `spacy_nlp`. Calling the nlp object on a string of text will return a processed Doc. In particular, When you call `spacy_nlp` on a text, spaCy first tokenizes the text to produce a Doc object. The Doc is then processed in several different steps – this is also referred to as the processing pipeline. The pipeline used by the trained pipelines typically include a tagger, a lemmatizer, a parser and an entity recognizer. Each pipeline component returns the processed Doc, which is then passed on to the next component."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81842a3",
   "metadata": {},
   "source": [
    "<!--\n",
    "<div>\n",
    "<img src=\"../07-pictures/spacy_nlp_pipeline.png\" width=\"500\"/>\n",
    "</div>\n",
    "-->\n",
    "\n",
    "![pic](./pic/spacy_nlp_pipeline.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66aff962",
   "metadata": {},
   "source": [
    "*source: spaCy [on line documentation](https://spacy.io/usage/spacy-101)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d74f75a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "article = \"The fourth Wells account moving to another agency is the packaged paper-products division of Georgia-Pacific Corp., which arrived at Wells only last fall. Like Hertz and the History Channel, it is also leaving for an Omnicom-owned agency, the BBDO South unit of BBDO Worldwide. BBDO South in Atlanta, which handles corporate advertising for Georgia-Pacific, will assume additional duties for brands like Angel Soft toilet tissue and Sparkle paper towels, said Ken Haldin, a spokesman for Georgia-Pacific in Atlanta.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6a36d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original article: The fourth Wells account moving to another agency is the packaged paper-products division of Georgia-Pacific Corp., which arrived at Wells only last fall. Like Hertz and the History Channel, it is also leaving for an Omnicom-owned agency, the BBDO South unit of BBDO Worldwide. BBDO South in Atlanta, which handles corporate advertising for Georgia-Pacific, will assume additional duties for brands like Angel Soft toilet tissue and Sparkle paper towels, said Ken Haldin, a spokesman for Georgia-Pacific in Atlanta. \n",
      "\n",
      "Type : ORDINAL, Value : fourth\n",
      "Type : ORG, Value : Wells\n",
      "Type : ORG, Value : Georgia-Pacific Corp.\n",
      "Type : ORG, Value : Wells\n",
      "Type : PERSON, Value : Hertz\n",
      "Type : ORG, Value : the History Channel\n",
      "Type : ORG, Value : Omnicom\n",
      "Type : ORG, Value : BBDO South\n",
      "Type : ORG, Value : BBDO Worldwide\n",
      "Type : ORG, Value : BBDO South\n",
      "Type : GPE, Value : Atlanta\n",
      "Type : ORG, Value : Georgia-Pacific\n",
      "Type : ORG, Value : Angel Soft\n",
      "Type : PERSON, Value : Ken Haldin\n",
      "Type : ORG, Value : Georgia-Pacific\n",
      "Type : GPE, Value : Atlanta\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">The \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    fourth\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORDINAL</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Wells\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " account moving to another agency is the packaged paper-products division of \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Georgia-Pacific Corp.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", which arrived at \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Wells\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " only last fall. Like \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Hertz\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the History Channel\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", it is also leaving for an \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Omnicom\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       "-owned agency, the \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    BBDO South\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " unit of \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    BBDO Worldwide\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ". \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    BBDO South\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Atlanta\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", which handles corporate advertising for \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Georgia-Pacific\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", will assume additional duties for brands like \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Angel Soft\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " toilet tissue and Sparkle paper towels, said \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Ken Haldin\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", a spokesman for \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Georgia-Pacific\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Atlanta\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spacy_nlp = spacy.load(\"en_core_web_sm\")\n",
    "document  = spacy_nlp(article)\n",
    "print('Original article: %s \\n' % (article))\n",
    "for element in document.ents:\n",
    "    print('Type : %s, Value : %s' % (element.label_, element))\n",
    "\n",
    "spacy.displacy.render(spacy_nlp(article), style='ent', jupyter=True)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f358e215",
   "metadata": {},
   "source": [
    "Natural language processing applications are characterized by complex interdependent decisions that require large amounts of prior knowledge. In this case, as you can see, the system designed by Stanford did not achieve the same result as spaCy, but it is pure accident; in fact, a lot depends on how well the models have been trained and with how much data. For this reason, in case there is a need to perform a task like this, the best thing to do is to use multiple tools and compare the results, in order to find the best one in terms of performance and response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae4ff95",
   "metadata": {},
   "source": [
    "## References and Credits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4e7ef5",
   "metadata": {},
   "source": [
    "***Bird S. et al.***, \"*Natural Language Processing with Python*\" O'Reilly (2009)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c93f4c",
   "metadata": {},
   "source": [
    "***Bengfort B. et al.***, \"*Applied Text Analysis with Python*\" O'Reilly (2018)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": "4",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
